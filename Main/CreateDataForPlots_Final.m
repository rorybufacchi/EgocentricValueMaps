% -----
% USE:
% 
% Run this script to generate the data from scratch. 
% -----
%
% =========================================================================
%                              !!! WARNING !!!
% =========================================================================
% It is !!! HIGHLY RECOMMENDED !!! to either: 
%       --------------------------   
% A) To skip using this script, and instead just load the pre-computed 
%    outputs , which are available $$$ in the 'GeneratedData' sub-directory
% OR      -------------
% B) Run each section separately:
%    some sections can take a VERY VERY long time to run
%    (Yes,this code is extremely under-optimised in terms of runtime)
% =========================================================================
%

% First make sure the path is set right though
cFileName       = matlab.desktop.editor.getActiveFilename;
codePath        = cFileName(1:end-31);
addpath(        genpath(codePath));

% NOTE: Change this to the file path where you store the generated data,
% whether downloaded or generated by yourself
dataPath                    = 'C:\Users\Rory Bufacchi\Documents\Projects\DefenseAgent'; '';
if              isempty(dataPath)
     dataPath       = SetPathEgocentricMapsData();
end
cd(             dataPath);



%% Pre-ANN Model 1, part 1/2: Find performance effect network sizes for 
% ANNs that will constitute ANN MODEL 1 (Fig 2)

clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.gol.alSpR                 = 1;
s.gol.alSpC                 = [0 0];
s.gol.randSpr               = [2 2];
s.lp.b1Siz                  = 1e4;
s.wrld.size                 = [14 15];
s.wrld.resetType            = 'BottomTop_InfLR';
s.lmb.startCol              = 8;
s.fl.trainTable             = 1; 
s.fl.trainNet               = 1;
s.rl.maxRetr                = 50;
s.fl.perfWhileLearn         = 1;
s.prf.nRep                  = 10;
s.prf.skipBatches           = 2;
netSizes                    = {[9],[9 9],[9 9 9],[9 9 9 9],[9 9 9 9 9]};

% -------------------------------------------------------------------------
% Collect state transitions
[s w storedEps net Qtable] = RunRLfun(s);

% -------------------------------------------------------------------------
% Train neural networks
for iM = 1:length(netSizes)
    tic
    
    s.fl.newNet             = 1;
    s.fl.newTable           = 1;
    
    % Change specific settings of the model
    s.lp.netS               = netSizes{iM};

    % Relearn Q with different netsizes
    [net Qtable perf]       = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s              = s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf           = perf;
    bFld                    = 'Results\Performance\ProximityPosition_';
    svNm                    = [bFld 'Mults_of_9'];
    save(                   svNm,'ntRS','-v7.3');
    iM
    toc
end

%% Pre-ANN Model 1, part 2/2: Find performance effect network sizes for ANNs that
%  will constitute ANN MODEL 1, but have MULTIPLE LIMBS (Fig 2)
clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.gol.alSpR                 = 1;
s.gol.alSpC                 = [0 0];
s.gol.randSpr               = [2 2];
s.lp.b1Siz                  = 1e4;
s.lp.minExp                 = 4e7; % minumum number of actions before training sarts (big here so no training)
s.lp.dispA                  = 1e6; % only show action count very infreequently
s.wrld.size                 = [14 15];
s.wrld.resetType            = 'BottomTop_InfLR';
s.rp.maxActions             = 4e6 ;
s.act.bdyGoalRew            = 2;
s.lmb.startCol              = 8;
s.fl.trainTable             = 1; 
s.fl.trainNet               = 1;
s.rl.maxRetr                = 100;
s.fl.bdyMov                 = 1;
s.fl.perfWhileLearn         = 1;
s.prf.nRep                  = 10;
s.prf.skipBatches           = 5;

netSizes={[9],[9 9],[9 9 9],[9 9 9 9],[9 9 9 9 9],[9 9 9 9 9], ...
    9*ones(1,6),9*ones(1,7),9*ones(1,8),9*ones(1,9),9*ones(1,10),9*ones(1,11)};

% -------------------------------------------------------------------------
% Collect state transitions
[s w storedEps net Qtable] = RunRLfun(s);

% -------------------------------------------------------------------------
% Train neural networks
for iM = 1:length(netSizes)
    
    s.fl.newNet             = 1;
    s.fl.newTable           = 1;
    
    % Change specific settings of the model
    s.lp.netS               = netSizes{iM};

    % Relearn Q with different netsizes
    [net Qtable perf]       = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s              =s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf           = perf;  
    bFld                    = 'Results\Performance\ProximityPosition_';
    svNm                    = [bFld '2LIMBS_Mults_of_9'];
    save(                   svNm,'ntRS','-v7.3');
    
end

%% ANN Model 1, PART 1/3: running models for one limb
%  Data for plot 2abc about distance and position dependence, 
%  and the corresponding stats (Fig 2)
clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.gol.alSpR = 1;
s.gol.alSpC = [0 0];
s.gol.randSpr = [2 2];
s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr=100;
s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=5;
netSizes={[12 10 8 6],[9 9 9 9],[6 8 10 12]};

% -------------------------------------------------------------------------
% Run the model
 [s w storedEps net Qtable] = RunRLfun(s);
% -------------------------------------------------------------------------
% Run each type of model
for iM=1:length(netSizes)
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    
    
    [net Qtable] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store results Structure
    rS(iM).s=s; rS(iM).Qtable=Qtable; rS(iM).w=w; rS(iM).net=net;
    
    svNm='Results\ForFigures\Fig2abc_model1_1Limb.mat';
    try
        save(svNm,'rS','-v7.3')
    catch
        save(svNm,'rS')
    end
    iM
end

%% ANN Model 1, PART 2/3: running models for MULTIPLE limb
%  Data for plot 2abc about distance and position dependence, 
%  and the corresponding stats (Fig 2)
clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.gol.alSpR = 1;
s.gol.alSpC = [0 0];
s.gol.randSpr = [2 2];
s.lp.b1Siz = 1e4;
s.lp.minExp = 4e7; % minimum number of actions before training sarts (big here so no training)
s.lp.dispA = 1e6; % only show action count very infreequently
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.rp.maxActions = 4e6 ;
s.act.bdyGoalRew = 2;
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr=100;
s.fl.bdyMov = 1;

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=5;

netSizes={[13 11 9 7 5],[9 9 9 9 9],[5 7 9 11 13]};

% -------------------------------------------------------------------------
% Run the model
[s w storedEps net Qtable] = RunRLfun(s);
% -------------------------------------------------------------------------
% Run each type of model
for iM = 1:length(netSizes)
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};

    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s=s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf = perf;
    
    bFld = 'Results\ForFigures\';
    svNm = [bFld 'Fig2abc_model1_2Limbs.mat'];
    save(svNm,'ntRS','-v7.3');
    iM
end

%% ANN Model 1, PART 3/3: performing analyses (Fig 2)

% Load Model 1, one limb
load('Results\ForFigures\Fig2abc_model1_1Limb_Precomputed.mat');
tmprS = rS;
if ~isfield(tmprS,'perf')
    for iM=1:length(tmprS)
        tmprS(iM).perf = [];
    end
end
rS = tmprS;


% Load Model 1, multiple limbs
load('Results\ForFigures\Fig2abc_model1_2Limbs_Precomputed.mat');
if ~isfield(ntRS,'perf')
    for iM=1:length(tmprS)
        ntRS(iM).perf = [];
    end
end
tmprS(end+1:end+length(ntRS)) = ntRS;
rS = tmprS;

% Load Combo Model [model with all the bells and whistles]
load('Results\Performance\ComboModel_Precomputed.mat');
rS(length(rS)+1).s  = ntRS.s;
rS(end).w           = ntRS.w;
rS(end).net         = ntRS.net;
rS(end).perf        = ntRS.perf;

clear tmprS

% -------------------------------------------------------------------------
% Perform the analyses for all models
clear allNeurAct_PlusBody Q_PlusBody
for cM = 1:length(rS)
    
    s = rS(cM).s;
    w = rS(cM).w;
    net = rS(cM).net;
    Qtable = rS(cM).Qtable;
    
    s = DefaultSettings(s);
    s.plt.plotThrFl = 0;
    
    % settings for plot
    sFP = s;
    %     sFP.plt.lmbCol = 2:s.wrld.size(2)-1;
    sFP.plt.lmbCol              = 3:s.wrld.size(2)-2;
    sFP.plt.ON                  = 0;
    sFP.plt.sequentialLimbCols  = 0;
    sFP.plt.stimRow             = [3:size(w.world2D,1)-1];
    sFP.plt.stimCol             = [2:size(w.world2D,2)-1];
    sFP.plt.pltType             = 'Binned';
    
    if s.fl.bdyMov == 1
        clear Q_PlusBody allNeurAct_PlusBody
        for iBdyCol = 1:size(w.world2D,2)
            s.plt.bdyCol = iBdyCol;
            
            [Q_tmp, allNeurAct_tmp] = CalcNetOutput(s,w,net);
            Q_PlusBody(:,:,:,:,:,iBdyCol) = Q_tmp;
            allNeurAct_PlusBody(:,:,:,:,:,:,iBdyCol) = allNeurAct_tmp;
        end
        
        allNeurAct_MeanBody = nanmean(allNeurAct_PlusBody(:,:,:,:,:,:,2:size(w.world2D,2)-1),7);
        allNeurAct_MeanLimb = permute(nanmean( ...
            allNeurAct_PlusBody(:,:,:,2:size(w.world2D,2)-1,:,:,:),4),[1 2 3 7 5 6 4]);
        
        Q_MeanBody = nanmean(Q_PlusBody(:,:,:,:,:,2:size(w.world2D,2)-1),6);
        Q_MeanLimb = permute(nanmean(Q_PlusBody(:,2:size(w.world2D,2)-1,:,:,:,:),2),[1 6 3 4 5 2]);
        
        allNeurAct = allNeurAct_MeanBody;
        Q = Q_MeanBody;
        
        % This returns a correlation value for each (neuron, layer, limbcol)0.05;
        [rS(cM).bdy.rDistN, rS(cM).bdy.pDistN, rS(cM).bdy.hProxN, aD, rD, cD, ~, ~, rS(cM).bdy.rUDistN, rS(cM).bdy.rLDistN] = ...
            CalcDistCorr(sFP,w,permute(allNeurAct_MeanLimb,[3 4 1 2 5 6]));
        % and this for each action q-value
        [rS(cM).bdy.rDistQ, rS(cM).bdy.pDistQ, rS(cM).bdy.hProxQ, aDQ, rDQ, cDQ, ~, ~, rS(cM).bdy.rUDistQ, rS(cM).bdy.rLDistQ] = ...
            CalcDistCorr(sFP,w,Q_MeanLimb);
        
        % This returns a correlation value for each (neuron, layer, limbcol)0.05;
        [rS(cM).lmb.rDistN, rS(cM).lmb.pDistN, rS(cM).lmb.hProxN, aD, rD, cD, ~, ~, rS(cM).lmb.rUDistN, rS(cM).lmb.rLDistN] = ...
            CalcDistCorr(sFP,w,permute(allNeurAct_MeanBody,[3 4 1 2 5 6]));
        % and this for each action q-value
        [rS(cM).lmb.rDistQ, rS(cM).lmb.pDistQ, rS(cM).lmb.hProxQ, aDQ, rDQ, cDQ, ~, ~, rS(cM).lmb.rUDistQ, rS(cM).lmb.rLDistQ] = ...
            CalcDistCorr(sFP,w,Q_MeanBody);
    else
        [Q,allNeurAct] = CalcNetOutput(sFP,w,net);
    end
    
    % This returns a correlation value for each (neuron, layer, limbcol);
    [rS(cM).rDistN, rS(cM).pDistN, rS(cM).hProxN, aD, rD, cD, ~, ~, rS(cM).rUDistN, rS(cM).rLDistN] = ...
        CalcDistCorr(sFP,w,permute(allNeurAct,[3 4 1 2 5 6]));
    % and this for each action q-value
    [rS(cM).rDistQ, rS(cM).pDistQ, rS(cM).hProxQ, aDQ, rDQ, cDQ, ~, ~, rS(cM).rUDistQ, rS(cM).rLDistQ] = ...
        CalcDistCorr(sFP,w,Q);
    
    
    
    % Return neural activation for penultimate network
    if cM == 4
        QforPlot_MeanBody = Q_MeanBody;
        QforPlot_MeanLimb = Q_MeanLimb;
        neurActForPlot_MeanBody = permute(allNeurAct_MeanBody,[3 4 1 2 5 6]);
        neurActForPlot_MeanLimb = permute(allNeurAct_MeanLimb,[3 4 1 2 5 6]);
    end
    
    
    % Return neural activation for first only limb network
    if cM < 4
        neurActForPlot{cM} = permute(allNeurAct,[3 4 1 2 5 6]);
    else
        neurActForPlot{cM} = allNeurAct_PlusBody;
    end
    
end


% For all the models
[rMat] = DisplayProxStats(rS);

% -------------------------------------------------------------------------
% For only models with multiple limbs - split the appropriate structure up
iM2 = 1; iM3 = 1;
for iM=1:length(rS)
    % load any possible extra settings
    s = rS(iM).s;
    s = DefaultSettings(s);
    
    
    if s.fl.bdyMov == 1
        
        % -----------------------------------------------------------------
        % Make a new substructure for every limb, rSML.
        % AND also make a structure which includes all models, but each limb as
        % a separate model, rSAll
        sFields = fields(rS(iM).lmb);
        for iF = 1:length(sFields)
            rSML(iM2).(sFields{iF})        = rS(iM).lmb.(sFields{iF});
        end
        rSML(iM2).s      = rS(iM).s;
        rSML(iM2).Qtable = rS(iM).Qtable;
        rSML(iM2).w      = rS(iM).w;
        rSML(iM2).net    = rS(iM).net;
        
        sFields = fields(rSML(iM2));
        for iF = 1:length(sFields)
            rSAll(iM3).(sFields{iF})        = rSML(iM2).(sFields{iF});
        end
        
        iM2 = iM2 + 1;
        iM3 = iM3 + 1;
        
        sFields = fields(rS(iM).lmb);
        for iF = 1:length(sFields)
            rSML(iM2).(sFields{iF})        = rS(iM).bdy.(sFields{iF});
        end
        rSML(iM2).s      = rS(iM).s;
        rSML(iM2).Qtable = rS(iM).Qtable;
        rSML(iM2).w      = rS(iM).w;
        rSML(iM2).net    = rS(iM).net;
        
        sFields = fields(rSML(iM2));
        for iF = 1:length(sFields)
            rSAll(iM3).(sFields{iF})        = rSML(iM2).(sFields{iF});
        end
        
        iM2 = iM2 + 1;
        iM3 = iM3 + 1;
        
    else
        rSAll(iM3) = rS(iM);
        iM3 = iM3 + 1;
    end
end



% -------------------------------------------------------------------------
% FINAL RESULTS (more than in paper)

% So multiple limb results here
disp('-----------')
disp('Multiple limb results')
[rMat_ML] = DisplayProxStats(rSML);

disp('-----------')
disp('All results')
[rMat_All] = DisplayProxStats(rSAll);

disp('-----------')
disp('Q correlation with proximity')
disp(rMat_All.pDistQ)

disp([''])
disp(['Proportion of neurons that correlate with proximity'])
disp(rMat_All.propCorrNeur)
disp(['Av: ' num2str(nanmean(rMat_All.propCorrNeur)) '+-' num2str(nanstd(rMat_All.propCorrNeur)) ])

% also find proportion in last layer
for iM = 1:length(rSAll)
    llCorr(iM) = rMat_All.propCorrNeurPerLay(length(rSAll(iM).s.lp.netS),iM);
end
disp(['Last layer: ' num2str(nanmean(llCorr)) '+-' num2str(nanstd(llCorr)) ]);



% -------------------------------------------------------------------------
% Save variables necessary for outputting results [In Figure2_Final.m]
save('Results\ForFigures\Fig2abc_Results.mat','rSAll','rSML','neurActForPlot_MeanBody','neurActForPlot_MeanLimb','rS','rMat_All','rMat')





%% Pre-ANN Model 2: Find performance effect network sizes for ANNs that
%  will constitute ANN MODEL 2 (Fig 2)
% (i.e., find performance of different network sizes for velocity and
% direction dependence)
clear s rS ntRS rtRS ytRS olRS


% -------------------------------------------------------------------------
% Base settings
s.gol.alSpR = [1 2 3];
s.gol.alSpC = [-2 -1 0 1 2];
s.gol.randSpr = [2 2];
s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr=51;
% s.rl.maxRetr=1;
s.fl.hist = 1;

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=5;

netSizes={12.*ones(1,4),12.*ones(1,5),12.*ones(1,6), ...
    12.*ones(1,7),12.*ones(1,8),12.*ones(1,9), ...
    13.*ones(1,9),14.*ones(1,9)};

% Run the model and learn Q (table)
[s w storedEps net Qtable] = RunRLfun(s);

% Run each type of model
for iM = 1:length(netSizes)
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    
    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s=s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf = perf;
    
    bFld = 'Results\Performance\VelocityDirection_';
    svNm = [bFld 'Mults_of_12_51Batch'];
    save(svNm,'ntRS','-v7.3');
    
end


%% ANN Model 2, PART 1/2: running models 
%  Data for plot 2defg about velocity and direction dependence dependence, 
%  and the corresponding stats (Fig 2)

clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.gol.alSpR = [1 2 3];
s.gol.alSpC = [-2 -1 0 1 2];
s.gol.randSpr = [2 2];
s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr=51;
s.fl.hist = 1;

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=5;

netSizes={[8 9 10 11 12 13 14 15 16], 12.*ones(1,9), [16 15 14 13 12 11 10 9 8]};

% Run the model and learn Q (table)
[s w storedEps net Qtable] = RunRLfun(s);

% Run each type of model
for iM = 1:length(netSizes)
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    
    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    rS(iM).s=s; rS(iM).Qtable=Qtable; rS(iM).w=w; rS(iM).net=net;
    rS(iM).perf = perf;
    
    bFld = 'Results\ForFigures\';
    svNm = [bFld 'Fig2defg_model2_Base.mat'];
    save(svNm,'rS','-v7.3');
    
end


%% ANN Model 2, PART 2/2: performing analyses (Fig 2)
%  WARNING: this takes quite long to run

load('Results\ForFigures\Fig2defg_model2_Base_Precomputed.mat');
load('Results\Performance\ComboModel_Precomputed.mat');
rS(end+1) = ntRS;


% -------------------------------------------------------------------------
% First calculate to what extent Q values and neural activities correlate
% with distance

iM = 1;
sFP = DefaultSettings(rS(iM).s);

nLay = 1; maxN = 1;
for iM=1:length(rS)
    nLay = max([nLay, length(rS(iM).s.lp.netS)]);
    maxN = max([maxN, max(rS(iM).s.lp.netS)]);
end

% Loop through velocities (row lags) and calculate 'extent of PPS'
for iV = 1:3 %sFP.gol.alSpR
    rS_sepV(iV).rS = rS;
    rS_sepV(iV).V = iV;
    
    for iM = 1:length(rS)

        sFP = DefaultSettings(rS(iM).s);
        w = rS(iM).w;
        net = rS(iM).net;
        Qtable = rS(iM).Qtable;
        sFP.plt.rowLag = iV;
        sFP.plt.rowLims = [1.5 sFP.wrld.size(1)-0.5];
        sFP.plt.stimRow=[3:size(w.world2D,1)-3];
        
        
        % -----------------------------------------------------------------
        % Calculate the correlati-ness of each velocity, for Q-values and neural
        % activity
        sFP.plt.meanLimbCols = 1;
        sFP.plt.fitSigmoid = 0;
        sFP.plt.lmbCol = [2:(sFP.wrld.size(2)-1)];
        sFP.plt.stimCol= [2:(sFP.wrld.size(2)-1)] ;
        [Q,allNeurAct] = CalcNetOutput(sFP,w,net);
        
        [rS_sepV(iV).rS(iM).rDistQ, rS_sepV(iV).rS(iM).pDistQ, ...
            rS_sepV(iV).rS(iM).hProxQ] = ...
            CalcDistCorr(sFP,w,Q);
        
        [rS_sepV(iV).rS(iM).rDistN, rS_sepV(iV).rS(iM).pDistN, ...
            rS_sepV(iV).rS(iM).hProxN] = ...
            CalcDistCorr(sFP,w,permute(allNeurAct,[3 4 1 2 5 6]));

    end
end

% -------------------------------------------------------------------------
% Extract Q-values for all models, row lags and column lags

clear qDiffVec pCL rCL Qall

allColLags = [-2 -1 0 1 2];
allRowLags = [1 2 3];

Qall = NaN([14 15 14 15 9 length(allColLags) length(allRowLags) length(rS)]);

for iM = 1:length(rS)
for iRL = 1:length(allRowLags)
for iCL = 1:length(allColLags)
    
    sFP = DefaultSettings(rS(iM).s);
    w = rS(iM).w;
    net = rS(iM).net;
    sFP.plt.rowLag = allRowLags(iRL);
    sFP.plt.colLag = allColLags(iCL);
    sFP.plt.meanLimbCols = 1;
    sFP.plt.fitSigmoid = 0;
    sFP.plt.lmbCol = [2:(sFP.wrld.size(2)-1)];
    sFP.plt.stimCol= [2:(sFP.wrld.size(2)-1)] ;
    
    % Put the body under the limb if the body is included
    if sFP.act.numA == 3
        [Q,allNeurAct] = CalcNetOutput(sFP,w,net);
    elseif sFP.act.numA == 9
        clear Q
        for iLC = [1:(sFP.wrld.size(2))]
            sFP.plt.lmbCol = iLC;
            sFP.plt.bdyCol = iLC;
            [Qtmp,allNeurAct] = CalcNetOutput(sFP,w,net);
            Q(:,iLC,:,:,:) = Qtmp(:,iLC,:,:,:);
        end
    end
    
    Qall(size(Qall,1) + 1 - size(Q,1) : size(Qall,1),:,...
         size(Qall,3) + 1 - size(Q,3) : size(Qall,3),:,1:size(Q,5),iCL,iRL,iM) = Q;
    
end
end
end
% Shift Q-values to the center to make it all comparable
QallOrig = Qall;
[Qall]   = RecenterQNA(sFP,Qall);


% -------------------------------------------------------------------------
% Find Neural activation values for all models, row lags and column lags

clear qDiffVec pCL rCL

allColLags = rS(1).s.gol.alSpC;
allRowLags = rS(1).s.gol.alSpR;

for iM = 1:length(rS)
for iRL = 1:length(allRowLags)
for iCL = 1:length(allColLags)
    
    sFP = DefaultSettings(rS(iM).s);
    w = rS(iM).w;
    net = rS(iM).net;     
    sFP.plt.rowLag = allRowLags(iRL);   
    sFP.plt.colLag = allColLags(iCL);
    sFP.plt.meanLimbCols = 1;
    sFP.plt.fitSigmoid = 0;
    sFP.plt.lmbCol = [2:(sFP.wrld.size(2)-1)];
    sFP.plt.stimCol= [2:(sFP.wrld.size(2)-1)] ;
    [Q,allNeurAct] = CalcNetOutput(sFP,w,net);
    
    allNeurAct = permute(allNeurAct,[3 4 1 2 5 6]);
    
    if iM ==1 && iRL == 1 && iCL == 1
        nAall = nan([size(allNeurAct(:,:,:,:,1)) 11 50 length(allColLags) length(allRowLags) length(rS)]);
    end
    
    nAall(size(nAall,1) + 1 - size(allNeurAct,1) : size(nAall,1),:, ...
          size(nAall,3) + 1 - size(allNeurAct,3) : size(nAall,3),:, ...
          1:size(allNeurAct,5),1:size(allNeurAct,6),iCL,iRL,iM) = allNeurAct; 
    
end
end
end


% Shift neural activations to the center to make it all comparable
[nAall] = RecenterQNA(sFP,nAall);

% Average over row lags (stimulus velocities)
nAmean = nanmean(nAall,8);
% Average over rows, to find left-right peak position
nAline = nanmean(nAmean(1,:,:,:,:,:,:,1,:),3);

save('Results\ForFigures\Fig2defg_model2_Processed.mat', 'Qall', 'QallOrig', 'nAall', 'rS', 'rS_sepV', 'allColLags', 'allRowLags', '-v7.3');


%% %% Pre-ANN Model 3: Find performance effect of network sizes for ANNs that
%  will constitute ANN MODEL 3 (Fig 2)
%  (i.e., Find performance of different network sizes for tool use)
clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.fl.ToolChange=1;
s.lmb.ToolRows=[3];
s.lmb.ToolProb=0.5;

s.fl.hist = 0;
s.gol.alSpR = [1];
s.gol.alSpC =[0 0];
s.gol.randSpr = 0;
s.rp.maxActions = 2e6 ;

s.gol.randSpr = [2 2];
s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr=51;
s.fl.hist = 1;

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=5;

netSizes={12.*ones(1,3),12.*ones(1,4),12.*ones(1,5), ...
    12.*ones(1,6),12.*ones(1,7),12.*ones(1,8)};

% Run the model and learn Q
[s w storedEps net Qtable] = RunRLfun(s);

% Run each type of model
for iM = 1:length(netSizes)
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    
    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s=s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf = perf;
    
    bFld = 'Results\Performance\ToolUse_';
    svNm = [bFld 'Mults_of_12_51Batch'];
    save(svNm,'ntRS','-v7.3');
    
end

%% ANN Model 3, PART 1/2: running models (Fig 2)
%  Data for plot 2hij about tool use dependence, 
%  and the corresponding stats
clear s rS ntRS rtRS ytRS olRS

s.fl.ToolChange=1;
s.lmb.ToolRows=[5];
s.gol.alSpR = [1];
s.gol.alSpC =[0 0];
s.gol.randSpr = [2 2];
s.rp.maxActions = 2e6 ;
s.lp.minExp = 2e7;
s.lmb.ToolProb=0.5;

s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr=51;
s.fl.hist = 0;
s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=25;

netSizes={[8 10 12 14 16], 12.*ones(1,5), [16 14 12 10 8]};

% Run the environment
[s w storedEps ] = RunRLfun(s); 

% -------------------------------------------------------------------------
% Learn baseline dynamics without tool use
for iM = 1:length(netSizes)

    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};

    % ---------------------------------------------------------------------
    % Create stored epochs which do not receive reward for tool touch
    tmpS=cell2mat(storedEps.S);
    tmpPrvS = cell2mat(storedEps.prvS);
    % no tool stored eps
    sEnoTool.S = storedEps.S;
    sEnoTool.prvS = storedEps.prvmS; 
    sEnoTool.R = storedEps.R;
    % replace rewarded with non-rewarded case
    tmpInd = (sEnoTool.R > 1 & tmpS(:,4) == 1 & tmpPrvS(:, 2) < 9 );
    sEnoTool.R(tmpInd) = storedEps.R(tmpInd) - 2;
    sEnoTool.A = storedEps.A;

    % Learn the no-tool Q values (ie., Only Limb RS)
    s.fl.newNet=1;
    s.fl.trainTable=0; s.fl.trainNet=1;
    [net Qtable perf] = RelearnNNFun(s,w,sEnoTool); 
    % Store Only Limb network results Structure
    olRS(iM).s=s; olRS(iM).Qtable=Qtable; olRS(iM).w=w; olRS(iM).net=net;
    olRS(iM).perf = perf;
    
    % Store intermediate models
    bFld = 'Results\ForFigures\';
    svNm = [bFld 'Fig2hij_model3_Base.mat'];
    save(svNm,'olRS','-v7.3');
    
end
% Store models and data
bFld = 'Results\ForFigures\';
svNm = [bFld 'Fig2hij_model3_Base1.mat'];
save(svNm,'olRS','storedEps','-v7.3');


% -------------------------------------------------------------------------
% Data for response to reviewers: tool use

% Load network trained without tool
load('Fig2hij_model3_Base.mat')
s = olRS(1).s;
s.rl.maxRetr=1;
s.prf.skipBatches = 1;

% Start from the 'only limb' model, and learn tool use, storing the network
% after every single batch
for iM = 1:length(netSizes)
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};

    % Set the first network to be equal to the no-tool network
    incrToolRS(iM,1) = olRS(iM);

    for iBatch = 1:51
        % Then adapt the no-tool network, but with tool-use Q-values
        s.fl.newNet=0;
        [net Qtable perf] = RelearnNNFun(s,w,storedEps,incrToolRS(iM,iBatch - 1).net,Qtable);
        % Store Yes Tool network results Structure
        incrToolRS(iM,iBatch).s=s; incrToolRS(iM,iBatch).Qtable=Qtable; incrToolRS(iM,iBatch).w=w; incrToolRS(iM,iBatch).net=net;
        incrToolRS(iM,iBatch).perf = perf;

        % Store intermediate models
        bFld = 'Results\ForFigures\';
        svNm = [bFld 'Fig2hij_model3_Base2.mat'];
        save(svNm,'incrToolRS','olRS','-v7.3');

    end
    
end



%% ANN Model 3, PART 2/2: performing analyses (Fig 2)

load('Results\ForFigures\Fig2hij_model3_Base2_Precomputed.mat');

% -------------------------------------------------------------------------
% Find the number of peaks for each training step

% columns,layers,neurons,models,batches
npksN       = nan(12,5,16,3,51);
npksNnoTool = npksN;

tic
for iM = 1:size(incrToolRS,1)
    for iD = 1:size(incrToolRS,2)

        s = incrToolRS(iM,iD).s;
        w = incrToolRS(iM,iD).w;

        Qtable = incrToolRS(iM,iD).Qtable;

        s = DefaultSettings(s);
        s.plt.plotThrFl = 0;

        % settings for plot
        sFP = s;
        sFP.plt.lmbCol = 3:s.wrld.size(2)-2;
        sFP.plt.ON = 0;
        sFP.plt.sequentialLimbCols = 0;
        sFP.plt.stimRow = [3:size(w.world2D,1)-1];
        sFP.plt.stimCol = [2:size(w.world2D,2)-1];
        sFP.plt.pltType = 'Binned';

        % ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        % Calculate activity WRT limb location
        sFP.plt.DistFromTool = 0;
        sFP.plt.stimRow = [size(w.world2D,1)-6:size(w.world2D,1)-3]; 

        net = incrToolRS(iM,iD).net;
        sFP.plt.ToolPresent = 1;
        [Q,allNeurAct] = CalcNetOutput(sFP,w,net);
        [incrToolRS(iM,iD).rDistN, incrToolRS(iM,iD).pDistN, incrToolRS(iM,iD).hProxN, aD, rD, cD]    = CalcDistCorr(sFP,w,permute(allNeurAct,[3 4 1 2 5 6]));
        [incrToolRS(iM,iD).rDistQ, incrToolRS(iM,iD).pDistQ, incrToolRS(iM,iD).hProxQ, aDQ, rDQ, cDQ] = CalcDistCorr(sFP,w,Q);
        [incrToolRS(iM,iD).npks,   incrToolRS(iM,iD).pks,    incrToolRS(iM,iD).pkLocs]                = RowPeakFind(sFP,Q);
        Qall(:,:,:,:,:,iM,iD,1) = Q;

        for iL = 1:numel(s.lp.netS)
            for iN = 1:s.lp.netS(iL)
                npksN(:,iL,iN,iM,iD) = RowPeakFind(sFP, permute(allNeurAct(:,:,:,:,iL,iN),[3 4 1 2 5 6]));
            end
        end

        % Also calculate values for no tool
        sFP.plt.ToolPresent = 0;
        [QnoTool,allNeurAct] = CalcNetOutput(sFP,w,net);
        QallNoTool(:,:,:,:,:,iM,iD,1) = QnoTool;

        for iL = 1:numel(s.lp.netS)
            for iN = 1:s.lp.netS(iL)
                npksNnoTool(:,iL,iN,iM,iD) = RowPeakFind(sFP, permute(allNeurAct(:,:,:,:,iL,iN),[3 4 1 2 5 6]));
            end
        end
    end
end

save('Results\ForFigures\Fig2hij_model3_Processed.mat','Qall','QallNoTool','incrToolRS','npksN','npksNnoTool','-v7.3');

%% %% Pre-ANN Model 4: Find performance effect of network sizes for ANNs that
%  will constitute ANN MODEL 4 (Fig 2)
%  (i.e., Find performance of different network sizes for Valence Magnitude)
clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings

s.fl.thr = 1;
s.act.ThreatRew = 4;
s.act.DefaultMoveRew = -0.1;
s.fl.hist = 0;
s.gol.alSpR = [1];
s.gol.alSpC =[0 0];
s.gol.randSpr = [2 2];
s.thr.alSpR = [1];
s.thr.alSpC =[0 0];
s.thr.randSpr = [2 2];
s.rp.maxActions = 2e6 ;


s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.rl.maxRetr=51; 
s.fl.hist = 0;
s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=25;

netSizes={9.*ones(1,3),9.*ones(1,4),9.*ones(1,5), ...
    9.*ones(1,6),9.*ones(1,7),9.*ones(1,8)};

% Run the model and learn Q
[s w storedEps net Qtable] = RunRLfun(s);

% Run each type of model
for iM = 1:length(netSizes)
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    
    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s=s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf = perf;
    
    bFld = 'Results\Performance\ValenceMagnitude_';
    svNm = [bFld 'Mults_of_9_51Batch_plus2_plus4_rewards'];
    save(svNm,'ntRS','-v7.3');
end

%% ANN Model 4, PART 1/2: running models (Fig 2)
%  Data for plot 2klm about valence magnitude, 
%  and the corresponding stats
clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.fl.thr = 1;
s.act.GoalRew = 1;
s.act.ThreatRew = 10;
s.act.DefaultMoveRew = -0.1;
s.fl.hist = 0;
s.gol.alSpR = [1];
s.gol.alSpC =[0 0];
s.gol.randSpr = [0 0];
s.thr.alSpR = [1];
s.thr.alSpC =[0 0];
s.thr.randSpr = [2 2];
s.rp.maxActions = 2e6 ;

s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.rl.maxRetr=51; 
s.fl.hist = 0;
s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=25;

netSizes={[6 10 14 18],    [12 12 12 12 ],    [18 14 10 6]};

% Run the model and learn Q
[s w storedEps net Qtable] = RunRLfun(s);

% Run each type of model
for iM = 1:length(netSizes)
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
       
    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    rS(iM).s=s; rS(iM).Qtable=Qtable; rS(iM).w=w; rS(iM).net=net;
    rS(iM).perf = perf;
    
    bFld = 'Results\ForFigures\Valence\';
    svNm = [bFld 'Fig2klm_model4_Base.mat'];
    save(svNm,'rS','-v7.3');
end


%% ANN Model 4, PART 2/2: performing analyses (Fig 2)
load('Results\ForFigures\Valence\Fig2klm_model4_Base_Precomputed.mat');

% -------------------------------------------------------------------------
% Calculate Q values for different valence magnitudes (positive only)

% 0 indicates low valence, 1 indicates high valence (don't ask why - it's a
% fudge using the 'threat' as the high valence object with a high reward,
% but it's legitimate. It's just done in an ugly way).
aTP = [0 1];

for iM = 1:length(rS)
    for iV = 1:length(aTP)
        vRS(iM,iV)=rS(iM);
    end
end

Q = nan([14 15 14 15 9 length(rS) 2]);
for iM = 1:length(rS)
    
    % loop through valence
    for iV = 1:length(aTP)
        
        sFP = DefaultSettings(rS(iM).s);
        w = rS(iM).w;
        net = rS(iM).net;
        Qtable = rS(iM).Qtable;
        
        sFP.plt.meanLimbCols = 1;
        sFP.plt.lmbCol = [2:(sFP.wrld.size(2)-1)];
        sFP.plt.stimCol= [2:(sFP.wrld.size(2)-1)] ;
        sFP.plt.stimRow = [3:size(w.world2D,1)-1];
            
        sFP.plt.plotThrFl = aTP(iV);
        
        [qTmp,tmpNA] = CalcNetOutput(sFP,w,net);
        
        [vRS(iM,iV).rDistN, vRS(iM,iV).pDistN, vRS(iM,iV).hProxN, aD, rD, cD] = CalcDistCorr(sFP,w,permute(tmpNA,[3 4 1 2 5 6]));
        [vRS(iM,iV).rDistQ, vRS(iM,iV).pDistQ, vRS(iM,iV).hProxQ, aDQ, rDQ, cDQ] = CalcDistCorr(sFP,w,qTmp);
        
        
        Q(size(Q,1) + 1 - size(qTmp,1) : size(Q,1),:,...
            size(Q,3) + 1 - size(qTmp,3) : size(Q,3),:,1:size(qTmp,5),iM,iV) = qTmp;
        
        if iM == 1 & iV == 1
            nAall = nan([size(tmpNA(:,:,:,:,1)) max(arrayfun(@(x) length(rS(x).s.lp.netS) , [1:length(rS)])) max(arrayfun(@(x) max(rS(x).s.lp.netS) , [1:length(rS)])) length(rS) 2]);
        end
        nAall(:,:,:,:,1:size(tmpNA,5),1:size(tmpNA,6),iM,iV) = permute(tmpNA,[3 4 1 2 5 6]);
        
    end
end

save('Results\ForFigures\Fig2klm_model4_Processed.mat','vRS','rS','aTP','Q','nAall','w','-v7.3');


%% %% Pre-ANN Model 5: Find performance effect of network sizes for ANNs that
%  will constitute ANN MODEL 5 (Fig 3)
%  (i.e., Find performance of different network sizes for Valence Sign)

clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings

s.fl.thr = 1;
s.act.ThreatRew = -4;
s.act.DefaultMoveRew = -0.1;

s.fl.hist = 0;
s.gol.alSpR = [1];
s.gol.alSpC =[0 0];
s.gol.randSpr = [2 2];
s.thr.alSpR = [1];
s.thr.alSpC =[0 0];
s.thr.randSpr = [2 2];
s.rp.maxActions = 2e6 ;

s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.rl.maxRetr=51; 
s.fl.hist = 0;

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=25;

netSizes={9.*ones(1,3),9.*ones(1,4),9.*ones(1,5), ...
    9.*ones(1,6),9.*ones(1,7)};

% Run the model and learn Q
[s w storedEps net Qtable] = RunRLfun(s);

% Run each type of model
for iM = 1:length(netSizes)
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    
    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s=s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf = perf;
    
    bFld = 'Results\Performance\ValenceSign_';
    svNm = [bFld 'Mults_of_9_51Batch_plus2_minus4_rewards'];
    save(svNm,'ntRS','-v7.3');
    
end



%% ANN MODEL 5, PART 1/2: running models (Fig 3)
%  Data for plot 3c about creating successor features, 
%  and the corresponding stats

clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.gol.alSpR = 1;
s.gol.alSpC = [0 0];
s.gol.randSpr = [0 3];
s.thr.alSpR = 1;
s.thr.alSpC = [0 0];
s.thr.randSpr = [2 2];
s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr=51;

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=25;

s.fl.thr = 1;

netSizes={[12 10 8 6],[9 9 9 9],[6 8 10 12], ...
          [12 10 8 6],[9 9 9 9],[6 8 10 12]};
      
gRews = [2  2  2  0  0  0];
tRews = [0  0  0 -2 -2 -2];


% Run each type of model
for iM=1:length(netSizes)
    
    % -------------------------------------------------------------------------
    % Run the model
    s.act.GoalRew = gRews(iM);
    s.act.ThreatRew = tRews(iM);
    [s w storedEps net Qtable] = RunRLfun(s);
    % -------------------------------------------------------------------------
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    
    
    [net Qtable] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store results Structure
    rS(iM).s=s; rS(iM).w=w; rS(iM).net=net; 
    %rS(iM).Qtable=Qtable;
    if iM == length(netSizes)
        rS(iM).storedEps=storedEps;
    end

    bFld = 'Results\ForFigures\';
    svNm = [bFld 'Fig3_model5_Base.mat'];
    save(svNm,'rS','-v7.3');
end


%% ANN Model 5, PART 2/2: performing analyses (Fig 3)

load('Results\ForFigures\Fig3_model5_Base_Precomputed.mat');

% -------------------------------------------------------------------------
% Calculate a mean Q value - replaces all Q values below

clear QGl QThr


allNeurActThr = nan([rS(1).s.wrld.size rS(1).s.wrld.size ...
    max(arrayfun(@(x) numel(rS(x).s.lp.netS) ,[1:length(rS)] ) ) ...
    max(arrayfun(@(x) max(rS(x).s.lp.netS) ,[1:length(rS)] ) ) 2]);
allNeurActGl = allNeurActThr;
tic
for iRn=1:3

    net = rS(iRn).net;
    w   = rS(iRn).w;
    s   = DefaultSettings(rS(iRn).s);
    
    s.plt.plotThrFl=0;
    [QGl(:,:,:,:,:,iRn), tmptmpNA] = CalcNetOutput(s,w,net);
    allNeurActGl(:,:,:,:,1:size(tmptmpNA,5),1:size(tmptmpNA,6),iRn) = tmptmpNA;
    allNeurActGl(:,:,:,:,size(tmptmpNA,5)+1:end,size(tmptmpNA,6)+1:end,iRn) = NaN;
    
    net = rS(iRn+3).net;
    w   = rS(iRn+3).w;
    s   = DefaultSettings(rS(iRn+3).s);
    s.plt.plotThrFl=1;
    [QThr(:,:,:,:,:,iRn),tmptmpNA] = CalcNetOutput(s,w,net);
    allNeurActThr(:,:,:,:,1:size(tmptmpNA,5),1:size(tmptmpNA,6),iRn) = tmptmpNA;
    allNeurActThr(:,:,:,:,size(tmptmpNA,5)+1:end,size(tmptmpNA,6)+1:end,iRn) = NaN;
end


QGl2    = QGl;
QThr2   = QThr;
toc


lmbColsToTest=2:s.wrld.size(2)-1;


% -------------------------------------------------------------------------
% Loop through current Model, to find limb-specific activations

clear allNeurAct_PlusBody Q_PlusBody
for cM = 1:length(rS)
    
    s = rS(cM).s;
    w = rS(cM).w;
    net = rS(cM).net;
    
    s = DefaultSettings(s);
    s.plt.plotThrFl = 0;
    
    % settings for plot
    sFP = s;
    %     sFP.plt.lmbCol = 2:s.wrld.size(2)-1;
    sFP.plt.lmbCol = 3:s.wrld.size(2)-2;
    sFP.plt.ON = 0;
    sFP.plt.sequentialLimbCols = 0;
    sFP.plt.stimRow = [3:size(w.world2D,1)-1];
    sFP.plt.stimCol = [2:size(w.world2D,2)-1];
    sFP.plt.pltType = 'Binned';
 
    [Q,allNeurAct] = CalcNetOutput(sFP,w,net);
    
    % This returns a correlation value for each (neuron, layer, limbcol)0.05;
    [rS(cM).rDistN, rS(cM).pDistN, rS(cM).hProxN, aD, rD, cD] = ...
        CalcDistCorr(sFP,w,permute(allNeurAct,[3 4 1 2 5 6]));
    % and this for each action q-value
    [rS(cM).rDistQ, rS(cM).pDistQ, rS(cM).hProxQ, aDQ, rDQ, cDQ] = ...
        CalcDistCorr(sFP,w,Q);
    
end

% Calculate proximity correlations
[rMat] = DisplayProxStats(rS);

% -------------------------------------------------------------------------
% Calculate hit probabilities

lmbColsToTest=2:s.wrld.size(2)-1;
timeLagsToTest = [0 1 2 3 4 5];
maxTL = max(timeLagsToTest);
storedEps = rS(end).storedEps;
tmpS=cell2mat(storedEps.S);
hitProbMatAll=[];

for iTL = 1:length(timeLagsToTest)
    cTL = timeLagsToTest(iTL);
    for iLimbCol=1:length(lmbColsToTest)

        cLR=1; cLC=lmbColsToTest(iLimbCol); cGlR=12; cGlC=5;

        % There was a reward, and the hand was in a particular position
        hitBool = zeros([size(storedEps.R,1) iTL]);
        missBool = hitBool;
        ccTL = 0:cTL;
        for iiTL = 1:length(ccTL)
            hitBool(1 : end - (iiTL-1) , iiTL)  = abs(storedEps.R(iiTL:end )) > 1 ;
            missBool(1 : end - (iiTL-1) , iiTL) = abs(storedEps.R(iiTL:end )) < 1 ;
        end
        hitBool     = max(hitBool,[],2);
        missBool    = min(missBool,[],2); % $$$ MAYBE CHANGE THIS BACK TO MAX?
        hitS=tmpS(find( hitBool & ismember(tmpS(:,1),cLC)  )-1,:);

        % There NOT was a reward, and the hand was in a particular position
        tmpTimePs=find( missBool & ismember(tmpS(:,1),cLC)  )-1;
        tmpTimePs(tmpTimePs<=1)=[];
        missS=tmpS(tmpTimePs,:);

        xHitTmp=unique(hitS(:,4));
        yHitTmp=unique(hitS(:,5));
        xDimHit=numel(unique(xHitTmp));
        yDimHit=numel(unique(yHitTmp));

        xMissTmp=unique(missS(:,4));
        yMissTmp=unique(missS(:,5));
        xDimMiss=numel(unique(xMissTmp));
        yDimMiss=numel(unique(yMissTmp));

        [nMiss c]=hist3([missS(:,4),missS(:,5)],[xDimMiss,yDimMiss]);
        [nHit c]=hist3([hitS(:,4),hitS(:,5)],[xDimHit,yDimHit]);

        missMat=zeros(size(nMiss)+2);
        missMat(xMissTmp(1):xMissTmp(end),yMissTmp(1):yMissTmp(end)) = nMiss;
        hitMat=zeros(size(nMiss)+2);
        hitMat(xHitTmp,yHitTmp) = nHit;

        hitProbMat=hitMat./(hitMat+missMat);
        hitProbMat(isnan(hitProbMat))=0;

        hitProbMatAll(:,:,iLimbCol,iTL) = hitProbMat;
    end
end


save('Results\ForFigures\Fig3_model5_Processed_Precomputed.mat',...
    'lmbColsToTest','allNeurActGl','rMat','QGl','QThr','QGl2','QThr2','hitProbMatAll','-v7.3');


%% ANN MODEL 6, PART 1/3: running models (Fig 5, ExtDat Fig3)
%  Data for plot 5abcd about emerging sub-networks, 
%       and ALSO for Extended Data figure 3, if you change some settings
%  NOTE: this section can also be used to generate data for the
%  supplementary analyses with different artificial neuron types, as well
%  as for the smaller networks

clear s rS ntRS rtRS ytRS olRS

% ===================
% IMPORTANT TO CHANGE FOR RUNNING SUPPLEMENTARY ANALYSES (extended data Fig 3):
% % ---
% A) Set neuron type. All options used in supplementary methods:
%    'radbas'; %'softmax'; % 'logsig'; %'radbas'; % 'tribas'; %'tansig';  %'hardlim'; %'softmax'; %poslin'; % 'purelin' 
s.lp.neurType = 'tansig'; 
% % ---
% B) Set regularisation. All options used in supplementary methods:
%    % 'trainlmL1'; % 'trainlm';
s.lp.TrnFn = 'trainlmL1'; regName = 'L1';
% s.lp.TrnFn = 'trainlm'; regName = 'No';
% % ---
% C) Set network size. Big networks were used for most of the stats
%    reported in the main text. Small networks for supplementary and for
%    non-tansig artificial neuron types
netSizes={[6 8 10 12 14 16 18], [12 12 12 12 12 12 12], [18 16 14 12 10 8 6]}; netSize = 'BigNets';
% netSizes={[6 10 14 18], [12 12 12 12 ], [18 14 10 6]}; netSize = 'SmallNets';
% ===================

% -------------------------------------------------------------------------
% Base settings
for iRep = 1:15 
    
clear rS

s.fl.thr = 1;
s.act.ThreatRew = -2;
s.act.DefaultMoveRew = -0.1;


s.fl.hist = 0;
s.gol.alSpR = [1];
s.gol.alSpC =[0 0];
s.gol.randSpr = [0 0];
s.thr.alSpR = [1];
s.thr.alSpC =[0 0];
s.thr.randSpr = [2 2];
s.rp.maxActions = 4e6 ;


s.lp.minExp = 4e7;
s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;
s.rl.maxRetr=51; 
s.fl.hist = 0;

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=50;

% Run the model and learn Q
s = DefaultSettings(s);
[s w storedEps net Qtable] = RunRLfun(s);

% Run each type of model
for iM = 1:length(netSizes)
    
    s.fl.newNet     = 1;
    s.fl.newTable   = 1;
    s.fl.trainNet   = 1;
    s.act.numA      = 3;
    s.act.Name      = {'LEFT','STAY','RIGHT'};
    s               = DefaultSettings(s);

    % Change specific settings of the model
    s.lp.netS = netSizes{iM};
    % Relearn Q w
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    rS(iM).s=s; rS(iM).Qtable=Qtable; rS(iM).w=w; rS(iM).net=net;
    rS(iM).perf = perf;
    
    bFld = 'Results\ForFigures\Valence\';
    svNm = [bFld 'Regularization_' regName '_Fig5_ExtDatFig3_model6_Base' s.lp.neurType '_run' num2str(iRep) '_' netSize];

    save(svNm,'rS','-v7.3');

    iM
    iRep
end


end


%% ANN Model 6, PART 2/3: precomputing more variables (Fig 5, ExtDat Fig3)

% =======================
% OPTION A: main text, big networks
netSize     = 'BigNets';
neurTypes   = {'tansig'};
regTypes    = {'No'};

% % =======================
% % OPTION B: supplementary: multiple artificial neuron types
% netSize     = 'SmallNets';
% neurTypes   = {'tansig','logsig','softmax','poslin','purelin','tribas','radbas'};
% regTypes    = {'No','L1'};

% -------------------------------------------------------------------------
% - Identify network performance 
% - Identify network shape, 
% - Find neural activity 
% (warning: takes long)

tic
bFld = 'Results\ForFigures\Valence\';
for iTyp = 1:length(neurTypes) % Neuron type
    for iReg = 1:numel(regTypes) % Regularisation type

        % Prepare base files for loading
        cFiles = dir([bFld '*' regTypes{iReg} '*_' neurTypes{iTyp} '*' netSize '*.mat']);
        for iRun = 1:length(cFiles)
            load([bFld cFiles(iRun).name]);
            rSall(1:numel(rS),iRun,iTyp,iReg) = rS(:);

            for iM = 1:numel(rS)

                % ~~~~~~~~~~~~~~~~~~~~~
                % Network performance
                rewPerAct(iTyp,iReg,iRun,iM) = sum(rS(iM).perf.rewPerAct(end,:));

                % ~~~~~~~~~~~~~~~~~~~~~
                % Network structure
                try
                    rSall(iM,iRun,iTyp,iReg).s.plt.nPm = 1 ; % number of permutations $ 100
                    [rSall(iM,iRun,iTyp,iReg),allNetAR(iM,iRun,iTyp,iReg),allSumBinNPFdist(iM,iRun,iTyp,iReg),allSumBinNPFpmdist(:,iM,iRun,iTyp,iReg),allBinNPF(:,:,iM,iRun,iTyp,iReg),allBinNPFpm(:,:,:,iM,iRun,iTyp,iReg)] = AssessStructure(rSall(iM,iRun,iTyp,iReg));
                catch
                    if iTyp > 1
                        warning('Watch out, one of the structure assessments didnt work:')
                        iM
                        iRun
                        iTyp
                        iReg
                    end
                end
                
                % ~~~~~~~~~~~~~~~~~~~~~
                % Store neural activity separately (it's too big to keep in workspace)
                % This is needed for the PCA later
                s   = rS(iM).s;
                net = rS(iM).net;
                w   = rS(iM).w;
   
                for thrR = 1:s.wrld.size(1)
                    for thrC = 1:s.wrld.size(2)
                        s2=DefaultSettings(s);
                        s2.plt.ON = 0;
                        s2.plt.lmbCol=2:s.wrld.size(2)-1;
                        s2.plt.plotThrFl=0;
                        s2.plt.startLayer = 1;
                        s2.plt.stopLayer  = numel(s2.lp.netS);
                        s2.plt.otherStateVars = [thrR thrC];
                        [tmpQ,tmpNeurAct] = CalcNetOutput(s2,w,net);
                        % Initialise activations
                        if iM == 1 && thrR == 1 && thrC == 1
                            glByThrAct = nan( size(tmpNeurAct,[1 2 2 5 6 3 4]) - [0 0 2 0 0 0 0] );
                        end
                        % stimrow, stimcol, limbrow, limbcol
                        glByThrAct(:,:,:,:,1:size(tmpNeurAct,6),thrR,thrC) = ...
                            squeeze(nanmean(tmpNeurAct(:,:,:,s2.plt.lmbCol,s2.plt.startLayer:s2.plt.stopLayer,:),3));
                    end
                end

                save(['Results\ForFigures\Valence\NetworkActivations\' ...
                    'FullNetActivity_NeurType_' neurTypes{iTyp} '_RegType_' regTypes{iReg} '_Run_' num2str(iRun) '_NetArch_' num2str(iM) '_' netSize '.mat'], ...
                    'glByThrAct','-v7.3')

            end
        end
    end
end
toc

% -------------------------------------------------------------------------
% Save  generated data
bFld = 'Results\ForFigures\Valence\';
svNm = [bFld 'Fig5_ExtDatFig3_model6_Base_PreAnalysis_' netSize '.mat'];
save(svNm, 'rSall', 'allNetAR', 'rewPerAct' , '-v7.3');



%% ANN Model 6, PART 3/3: Performing analyses (Fig 5, ExtDat Fig3)

% =======================
% OPTION A: main text, big networks
netSize     = 'BigNets';
neurTypes   = {'tansig'};
regTypes    = {'No'};

% % =======================
% % OPTION B: supplementary: multiple artificial neuron types
% netSize     = 'SmallNets';
% neurTypes   = {'tansig','logsig','softmax','poslin','purelin','tribas','radbas'};
% regTypes    = {'No','L1'};


load(['Results\ForFigures\Valence\Fig5_ExtDatFig3_model6_Base_PreAnalysis_' netSize '_Precomputed.mat']);


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
% Calculate necessary variables for performing the LMEs

tic
clear layNum layNumDiff nodeDists stimPrefSim
for iTyp = 1:length(neurTypes);
for iReg = 1:length(regTypes);
for iM = 1:size(allNetAR,1)
    for iRun = 1:size(allNetAR,2)

    netAR   = allNetAR(iM,iRun,iTyp,iReg);
    rS      = rSall(iM,iRun,iTyp,iReg);

    % Find performance of network
    allPerf(iM,iRun,iTyp,iReg) = sum(rS(1).perf.rewPerAct(end,:));
    allNetW(iM,iRun,iTyp,iReg) = rS(1).s.lp.netS(end);

    % Make an indicator of layer depth
    tmp = netAR.A_B_rat';
    layNumTmp = [1:size(netAR.A_B_rat,1)] .*    ones(size(tmp));
    layNumTmp = layNumTmp(:);
    layNumTmp(isnan(tmp(:))) = [];
    layNum(:,iM,iRun,iTyp,iReg) = layNumTmp;
    layNumDiff(:,:,iM,iRun,iTyp,iReg)  = abs(bsxfun(@minus, layNumTmp', layNumTmp));

    A_B_rat = NanRemFlatten(netAR.A_B_rat'); classType = 'rat';
    G = netAR.G;

    p = plot(G,'Layout','force','WeightEffect','inverse','UseGravity','on');

    % First make a distance matrix
    for iNode=1:size(G.Nodes,1)
        nodeDists(:,iNode,iM,iRun,iTyp,iReg)    = sqrt(sum(([p.XData(iNode),p.YData(iNode)] - [p.XData(:),p.YData(:)])'.^2));
    end

    % Then make a stimulus preference matrix
    stimPrefSimTmp = 1 - abs(A_B_rat - A_B_rat');
    stimPrefSim(:,:,iM,iRun,iTyp,iReg)  = stimPrefSimTmp;

    % Calculate the correlation between the two
    nodeDistsTmp = nodeDists(:,:,iM,iRun,iTyp,iReg);
    [rho(iM,iRun,iTyp,iReg) pval(iM,iRun,iTyp,iReg)] = corr(stimPrefSimTmp(nodeDistsTmp ~= 0),nodeDistsTmp(nodeDistsTmp ~= 0));

    end
end
end
end
toc

% pvalTmp = pval(:,1:15,:,:);
pvalTmp = pval(:,1:15,1,1);
[pValThr, pValcor, pValAdj] = fdr(pvalTmp(:));

fprintf('%i out of %i networks show structure using this metric (not used in final paper) \n', sum(pValAdj(:) < 0.05), numel(pValAdj));

disp(['mean rho: ' num2str(nanmean(rho(:))) ' +- ' num2str(nanstd(rho(:))) 'SD'])


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
% Perform LME for all models independently, accounting for layer differences

overallModelNum    = repmat(  permute( reshape( 1:numel(allNetAR) , size(allNetAR) ), [5 6 1 2 3 4]) , [size(stimPrefSim,[1 2]) 1 1 1 1]);
layNumOrg   = repmat(permute(layNum,[1 6 2 3 4 5]) , [1 size(layNum,1) 1 1]);

allPerfRep  = repmat(permute(allPerf,[5 6 1 2 3 4 ]) , [size(layNumOrg,[1 2]) 1 1 1 1]);
allNetWRep  = repmat(permute(allNetW,[5 6 1 2 3 4 ]) , [size(layNumOrg,[1 2]) 1 1 1 1]);

allegedNoStructure      = pValAdj > 0.05;
allegedNoStructureRep   = repmat(permute(allegedNoStructure,[5 6 1 2 3 4 ]) , [size(layNumOrg,[1 2]) 1 1 1 1]);

% iM,iRun,iTyp,iReg
allMod = arrayfun(@(i) i .* ones(size(layNumOrg(:,:,1,:,:,:)) ) , 1:size(layNumOrg,3) ,'UniformOutput', false );
allMod = cat(3,allMod{:});
allRun = arrayfun(@(i) i .* ones(size(layNumOrg(:,:,:,1,:,:)) ) , 1:size(layNumOrg,4) ,'UniformOutput', false );
allRun = cat(4,allRun{:});
allTyp = arrayfun(@(i) i .* ones(size(layNumOrg(:,:,:,:,1,:)) ) , 1:size(layNumOrg,5) ,'UniformOutput', false);
allTyp = cat(5,allTyp{:});
allReg = arrayfun(@(i) i .* ones(size(layNumOrg(:,:,:,:,:,1)) ) , 1:size(layNumOrg,6) ,'UniformOutput', false );
allReg = cat(6,allReg{:});

tbl                             = table(stimPrefSim(:));
tbl.Properties.VariableNames    = {'StimulusPreferenceSimilarity'};
tbl.nodeDists                   = nodeDists(:);
tbl.layNumDiff                  = categorical(abs(layNumDiff(:)));
tbl.layNumOrg                   = layNumOrg(:);
tbl.modelNum                    = overallModelNum(:);
tbl.allPerf                     = allPerfRep(:);
tbl.allNetW                     = categorical(allNetWRep(:));
tbl.allegedNoStructure          = allegedNoStructureRep(:);
tbl.allMod                      = allMod(:);
tbl.allRun                      = allRun(:);
tbl.allTyp                      = allTyp(:);
tbl.allReg                      = allReg(:);


tmpTbl = tbl(tbl.nodeDists ~= 0, :);

% Run LME for each network
tic
for iTyp = 1:length(neurTypes);
for iReg = 1:length(regTypes);
for iM = 1:size(allNetAR,1)
    for iRun = 1:size(allNetAR,2)
    tmpTbl          = tbl(tbl.nodeDists ~= 0 & ...
                          tbl.allMod == iM   & ...
                          tbl.allRun == iRun   & ...
                          tbl.allReg == iReg   & ...
                          tbl.allTyp == iTyp   ,:);

    tmpTbl.allMod = categorical(tmpTbl.allMod);
    tmpTbl.allRun = categorical(tmpTbl.allRun);
    tmpTbl.allTyp = categorical(tmpTbl.allTyp);
    tmpTbl.allReg = categorical(tmpTbl.allReg);

    tmpLme          = fitlme(tmpTbl,'StimulusPreferenceSimilarity ~ layNumDiff:nodeDists + nodeDists + (1|modelNum)');

    tmpAnova                = anova(tmpLme);
    estimateDistEff(iM,iRun,iTyp,iReg)     = tmpLme.Coefficients{2,2};
    estimateStandErr(iM,iRun,iTyp,iReg)    = tmpLme.Coefficients{2,3};
    tStatsStructure(iM,iRun,iTyp,iReg)     = tmpLme.Coefficients{2,4};
    pNodeDists(iM,iRun,iTyp,iReg)          = tmpLme.Coefficients{2,6};


    end
end
end
end
toc

pTmp        = pNodeDists(:,1:15);
estTmp      = estimateDistEff(:,1:15);
estErrTmp   = estimateStandErr(:,1:15);
tStatTmp    = tStatsStructure(:,1:15); 

[pValThr, pValcor, pValLMEAdj] = fdr(pTmp(:));


fprintf('%i out of %i networks show structure using this metric\n', sum(pValLMEAdj(:) < 0.05), numel(pValLMEAdj));

dimsToSum = [1 2];

disp('Weighted mean tstat of node distance')
meanEffs = mean(tStatTmp(:))
disp('standard deviation of tstat of node distance')
sdEffs   = std(tStatTmp(:))


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
% Perform PCA on all networks, project onto explanatory variables, calculate angle of best projection and perform permutation testing

% set number of permutations
nPm = 1000;

% Load one file for the format

load(['Results\ForFigures\DimensionReduction\NetworkActivations\' ...
                    'FullNetActivity_NeurType_' neurTypes{1} '_RegType_' regTypes{1} '_Run_' num2str(1) '_NetArch_' num2str(1) '_' netSize '.mat']);


% Create 'explanatory variables'
thrRow = repmat( (1:s.wrld.size(1))' , [1, size(glByThrAct,[1 2 3 7]) ] );
thrRow = permute(thrRow, [2 3 4 1 5]);
thrCol = repmat( (1:s.wrld.size(2))' , [1, size(glByThrAct,[1 2 3 6]) ] );
thrCol = permute(thrCol, [2 3 4 5 1]);

golRow = repmat( (1:s.wrld.size(1))' , [1, size(glByThrAct,[2 3 6 7]) ] );
golRow = permute(golRow, [1 2 3 4 5]);
golCol = repmat( (1:s.wrld.size(2))' , [1, size(glByThrAct,[1 3 6 7]) ] );
golCol = permute(golCol, [2 1 3 4 5]);

lmbCol = repmat( (2:s.wrld.size(2)-1)' , [1, size(glByThrAct,[1 2 6 7]) ] );
lmbCol = permute(lmbCol, [2 3 1 4 5]);

golDist = sqrt((golCol - lmbCol).^2 + (golRow - w.lmb.row).^2);
thrDist = sqrt((thrCol - lmbCol).^2 + (thrRow - w.lmb.row).^2);

minStimDist = min(cat(6,golDist,thrDist),[],6);
maxStimDist = max(cat(6,golDist,thrDist),[],6);
avStimDist  = mean(cat(6,golDist,thrDist),6);



% Select the particalar comparisons and PCA dimensions of interest
% (vestigial)
iV = 1;
iD = 1;

% Create 'explanatory variables'
s      = rSall(1).s;
thrRow = repmat( (1:s.wrld.size(1))' , [1, size(glByThrAct,[1 2 3 7]) ] );
thrRow = permute(thrRow, [2 3 4 1 5]);
golRow = repmat( (1:s.wrld.size(1))' , [1, size(glByThrAct,[2 3 6 7]) ] );
golRow = permute(golRow, [1 2 3 4 5]);

tic
for iTyp = 1:length(neurTypes)
for iReg = 1:length(regTypes)
for iRun = 1:15
for iM = 1:size(rSall,1)

load(['Results\ForFigures\DimensionReduction\NetworkActivations\' ...
          'FullNetActivity_NeurType_' neurTypes{iTyp} '_RegType_' regTypes{iReg} '_Run_' num2str(iRun) '_NetArch_' num2str(iM) '_' netSize '.mat']);

    % Create flattened activations for PCA
    glByThrActFlat = permute(glByThrAct,[1 2 3 6 7 4 5]);
    unwrapSize     = size(glByThrActFlat);
    glByThrActFlat = permute(glByThrActFlat(:,:,:,:,:,:),[6 1 2 3 4 5]);
    glByThrActFlat = glByThrActFlat(:,:);

    % Perform PCA
    [coeff,score,~,~,explained] = pca(glByThrActFlat( ~isnan(glByThrActFlat(:,1)),:)');
    % Replace with zero if the PCA doesn't work
    if size(score,2) == 0
        score = zeros([size(glByThrActFlat,2) 3]);
    end

    % Downsample more here than when creating the basis vectors, because
    % otherwise we end up with a stupidly large array of vectors between
    % states
    tmpScore = reshape(score', [size(score,2) unwrapSize(1:5)] );
    tmpScore = tmpScore(:,:,2:2:end,1:2:end,:,2:2:end);
    tmpScore = tmpScore(:,:)';

    % Extract attributes of interest for given state
    allVars     = {'thrRow','thrCol','lmbCol','lmbCol','golDist',...
               'minStimDist','minStimDist'};
    allVarsComp = {'golRow','golCol','thrCol','golCol','thrDist',...
               'maxStimDist','avStimDist'};
    eval(['tmpCl    = ' allVars{iV} '(:,2:2:end,1:2:end,:,2:2:end);']);
    eval(['tmpSz    = ' allVarsComp{iV} '(:,2:2:end,1:2:end,:,2:2:end);']);
    tmpCl    = tmpCl(:);
    tmpSz    = tmpSz(:);

    dS = 1; dE = 3;
    clear dotProdPm

    % Find vector that best explains the variable of interest A
    [b1(:,iD,iV,iTyp,iReg,iRun,iM), b1Int, ~, ~, stats] = regress(tmpCl, [tmpScore(:,dS:dE), ones(size(tmpScore, 1), 1)]);
    % Find vector that best explains the variable of interest B
    [b2(:,iD,iV,iTyp,iReg,iRun,iM), b2Int, ~, ~, stats] = regress(tmpSz, [tmpScore(:,dS:dE), ones(size(tmpScore, 1), 1)]);
    % Calculate dot and (non-normalised) cross-products
    dotProdBase = sum( b1(1:3,iD,iV,iTyp,iReg,iRun,iM)./norm(b1(1:3,iD,iV,iTyp,iReg,iRun,iM)) .* ...
        b2(1:3,iD,iV,iTyp,iReg,iRun,iM)./norm(b2(1:3,iD,iV,iTyp,iReg,iRun,iM)) );  
    crossProdBase = norm(cross( b1(1:3,iD,iV,iTyp,iReg,iRun,iM) , ...
        b2(1:3,iD,iV,iTyp,iReg,iRun,iM) ));
           
    % Perform permutations
    for iPm = 1:nPm
        permOrd = randperm(size(tmpCl,1));

        % Find vector that best explains the variable of interest A
        [b1Tmp, ~, ~, ~, stats] = regress(tmpCl(permOrd), [tmpScore(:,dS:dE), ones(size(tmpScore, 1), 1)]);

        % Find vector that best explains the variable of interest B        
        [b2Tmp, ~, ~, ~, stats] = regress(tmpSz(permOrd), [tmpScore(:,dS:dE), ones(size(tmpScore, 1), 1)]);

        % Calculate dot and (non-normalised) cross-products
        dotProdPm(iPm)  = sum(b1Tmp(1:3)./norm(b1Tmp(1:3)) .* b2Tmp(1:3)./norm(b2Tmp(1:3)));
        crossProdPm(iPm) = norm(cross(b1Tmp(1:3) , b2Tmp(1:3)));
    end

    % Perform bootstrapping
    for iPm = 1:nPm
        permOrd = randi(size(tmpCl,1),size(tmpCl));

        % Find vector that best explains the variable of interest A
        [b1Tmp, ~, ~, ~, stats] = regress(tmpCl(permOrd), [tmpScore(permOrd,dS:dE), ones(size(tmpScore, 1), 1)]);
              % Find vector that best explains the variable of interest B
        [b2Tmp, ~, ~, ~, stats] = regress(tmpSz(permOrd), [tmpScore(permOrd,dS:dE), ones(size(tmpScore, 1), 1)]);

        % Calculate dot and (non-normalised) cross-products
        dotProdBs(iPm)  = sum(b1Tmp(1:3)./norm(b1Tmp(1:3)) .* b2Tmp(1:3)./norm(b2Tmp(1:3)));
        crossProdBs(iPm) = norm(cross(b1Tmp(1:3) , b2Tmp(1:3)));
    end
    
    toc
    
    
    alldotProd(iM,iRun,iTyp,iReg,iD,iV)   = dotProdBase;
    allcrossProd(iM,iRun,iTyp,iReg,iD,iV) = crossProdBase;
    
    pDotProd(iM,iRun,iTyp,iReg,iD,iV)    = sum( abs(dotProdBase)   - abs(dotProdPm) > 0) ./ nPm;
    pCrossProd(iM,iRun,iTyp,iReg,iD,iV)  = sum( abs(crossProdBase) - abs(crossProdPm) < 0) ./ nPm;
    
    alldotProdPm(iM,iRun,iTyp,iReg,iD,iV,:)   = dotProdPm;
    allcrossProdPm(iM,iRun,iTyp,iReg,iD,iV,:) = crossProdPm;
    
    alldotProdBs(iM,iRun,iTyp,iReg,iD,iV,:)   = dotProdBs;
    allcrossProdBs(iM,iRun,iTyp,iReg,iD,iV,:) = crossProdBs;      
toc
end
end
end
end
toc


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
% Create novel tasks and calculate corresponding Q-values

% --------------------------------------------------------------
% Run a quick dummy agent to get the world-structure 'w' 
% (Needed for plotting later)
s.rp.maxActions                 = 10; 
s.wrld.size                     = [14 15];
s                               = DefaultSettings(s);
[~, w]                          = RunRLfun(s);
s                               = DefaultSettings(s);

% --------------------------------------------------------------
% Main settings
s.clc.maximiseSimilarityType = 'OverallQ'; %'WinningQ' ; %'ChosenAction'; % 'OverallQ'
allGammas = [0.7];
iGamma = 1;
s.clc.RewardBehindSurfaceFl = 0;
s.clc.checkCollisionFl      = 0;
s.wrld.size = [14 15 1];

% settings for plot
sFP=s;
sFP.plt.lmbRow = s.wrld.size(1)-2;
sFP.plt.rowLims=[6.5 s.wrld.size(1)-1.5];
sFP.plt.colLims=[3.5 s.wrld.size(2)-3.5];
sFP.plt.cBarFl=0;
sFP.plt.meanLimbCols=1;
sFP=DefaultSettings(sFP);
sFP.plt.axesVis=1;
fS.gridXstart = -4.5;
fS.gridXstep = 1;
fS.gridYstart = 3.5;
fS.gridYstep = 1;
sFP.plt.lmbCol=8;

% Data fitting variables
s.clc.startRew =  1;
s.clc.startSR = 12;
s.clc.startSC =  8;
s.clc.startSZ =  1;
s.clc.nearPos = [s.wrld.size(1)-0 8 1]';
s.clc.nReps = 1;
s.clc.stepUpdateFl = 0;
s.clc.nSteps = 1;

s.clc.gammaVal   = allGammas(iGamma);
s.clc.baseVel    = [1 0 0];

% Random stimulus dynamics (here: none)
rSpr = 0;
rSprPr = 1;
cSpr = 0;
cSprPr = 1;
zSpr = 0; 
zSprPr = 1;
% x y z, Specify deterministic stimulus dynamics
s.clc.stimDynams =     @(pos) pos + s.clc.baseVel; % For approaching, set speed positive
s.clc.randSpread =     {rSpr cSpr zSpr}; % Put a little biit of x and z variability in? Kind of arbitrary
s.clc.spreadProb =     {rSprPr cSprPr  zSprPr}; % x y z, probabilities of spread
% Random sensory uncertainties
s.clc.sensSpread = {[0] , [0] , [0]};
s.clc.sensProb   = {[1] , [1] , [1]};
% Consequences of actions
s.clc.actConsequence = ...
    [0  0  0 ; ... % action 1 stay
    0  1  0 ; ... % action 2 left
    0 -1  0];     % action 3 right

fS.cAxis = allGammas(iGamma) .* [-1 1] .* max(cSprPr);

clear baseTasks

% --------------------------------------------------------------
% Create Q-values for policies and tasks

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% BASE TASKS

% POLICY: grab goal
% --------------------------------
% Task: grab goal
s.clc.useAltPolicyFl = 0;
s.clc.startRew = 1;
polGtaskGQtmp = CalcQDirect(s);
polGtaskGQ = repmat(permute(polGtaskGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{1,1}.allQ = polGtaskGQ;
baseTasks{1,1}.name = 'Pol: Goal, Task: Goal';

% Task: avoid threat
polGtaskTQ = -polGtaskGQ;
baseTasks{1,2}.allQ = polGtaskTQ;
baseTasks{1,2}.name = 'Pol: Goal, Task: Threat';

% Task: wide goal
sTmp = s;
sTmp.clc.useAltPolicyFl = 1;
sTmp.clc.startSC =  [ 7  8  9];
sTmp.clc.startSR =  [12 12 12];
sTmp.clc.startSZ =  [ 1  1  1];
sTmp.clc.startRew = 1;
polGtaskWideGQtmp = CalcQDirect(sTmp, [], polGtaskGQtmp);
polGtaskWideGQ = repmat(permute(polGtaskWideGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{1,3}.allQ = polGtaskWideGQ;
baseTasks{1,3}.name = 'Pol: Goal,Task: Wide Goal';


% POLICY: avoid threat
% --------------------------------
% Task: avoid threat
s.clc.startRew = -1;
polTtaskTQtmp = CalcQDirect(s);
polTtaskTQ = repmat(permute(polTtaskTQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{2,2}.allQ = polTtaskTQ;
baseTasks{2,2}.name = 'Pol: Threat, Task: Goal';

% Task: grab goal
polTtaskGQ = -polTtaskTQ;
baseTasks{2,1}.allQ = polTtaskGQ;
baseTasks{2,1}.name = 'Pol: Threat, Task: Threat';


% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% NOVEL TASKS

% Task: wide goal
sTmp = s;
sTmp.clc.useAltPolicyFl = 1;
sTmp.clc.startSC =  [ 7  8  9];
sTmp.clc.startSR =  [12 12 12];
sTmp.clc.startSZ =  [ 1  1  1];
sTmp.clc.startRew = 1;
polTtaskWideGQtmp = CalcQDirect(sTmp, [], polTtaskTQtmp);
polTtaskWideGQ = repmat(permute(polTtaskWideGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{2,3}.allQ = polTtaskWideGQ ; %widepolGtaskTQ(:,:,:,:,1);
baseTasks{2,3}.name = 'Pol: Threat,Task: Wide Goal';


% POLICY: grab WIDE goal
% --------------------------------
% Task: wide goal
sTmp = s;
sTmp.clc.useAltPolicyFl = 0;
sTmp.clc.startSC =  [ 7  8  9];
sTmp.clc.startSR =  [12 12 12];
sTmp.clc.startSZ =  [ 1  1  1];
sTmp.clc.startRew = 1;
polWideGtaskWideGQtmp = CalcQDirect(sTmp);
polWideGtaskWideGQ = repmat(permute(polWideGtaskWideGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{3,3}.allQ = polWideGtaskWideGQ;
baseTasks{3,3}.name = 'Pol: Wide Goal,Task: WideGoal';


% Task: grab goal
s.clc.useAltPolicyFl = 1;
s.clc.startRew = 1;
polWideGtaskGQtmp = CalcQDirect(s,[], polWideGtaskWideGQtmp);
polWideGtaskGQ = repmat(permute(polWideGtaskGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{3,1}.allQ = polWideGtaskGQ;
baseTasks{3,1}.name = 'Pol: Wide Goal, Task: Goal';

% Task: avoid threat
s.clc.useAltPolicyFl = 1;
s.clc.startRew = -1;
polWideGtaskTQtmp = CalcQDirect(s,[], polWideGtaskWideGQtmp);
polWideGtaskTQ = repmat(permute(polWideGtaskTQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{3,2}.allQ = polWideGtaskTQ;
baseTasks{3,2}.name = 'Pol: Wide Goal, Task: Threat';

% Reset alternative policy use
s.clc.useAltPolicyFl = 0;

% Create baseQ (matrix of basis tasks)
clear baseQ
for iPol = 1:size(baseTasks,1)
    qTmpForPol = arrayfun(@(iTask) baseTasks{iPol,iTask}.allQ, 1:size(baseTasks,2), 'UniformOutput', false);
    % row col row col act pol task
    baseQ(:,:,:,:,:,iPol,:) = cat(6,qTmpForPol{:});
end

% --------------------------------
% Task: 'stay in place for 1 timestep, then grab'
stationaryQ              = zeros(size(polGtaskGQ(:,:,:,:,:)));
stationaryQ(:,:,1:13,:,:)  = allGammas(iGamma).^1 .* polGtaskGQ(:,:,2:14,:,:);
stationaryQ(:,:,6:11,8,:)  = polGtaskGQ(:,:,6:11,8,:);
tasksToRecreate{1}.allQtoRecreate = stationaryQ; %stationaryQ(:,:,:,:,1);
tasksToRecreate{1}.name  = 'Stay1ThenGrab';

% --------------------------------
% Task: Avoid WIDE stimulus
s.clc.startSC =  [ 7  8  9];
s.clc.startSR =  [12 12 12];
s.clc.startSZ =  [ 1  1  1];

s.clc.startRew = -1;
widepolGtaskTQ = CalcQDirect(s);
widepolGtaskTQ = repmat(permute(widepolGtaskTQ,[4 5 2 3 1]),[14 15 1 1]);
tasksToRecreate{2}.allQtoRecreate = widepolGtaskTQ; %widepolGtaskTQ(:,:,:,:,1);
tasksToRecreate{2}.name = 'WideThreat';
s.clc.startSC =  8;

% --------------------------------
% Task: Pass 2 blocks by the right of the stimulus
stationaryQ              = zeros(size(polGtaskGQ(:,:,:,:,:)));
stationaryQ(:,:,:,3:15,:)  = polGtaskGQ(:,:,:,1:13,:);
stationaryQ(:,:,:,1,:)     = polGtaskGQ(:,:,:,15,:);
tasksToRecreate{3}.allQtoRecreate = stationaryQ;  %stationaryQ(:,:,:,:,1);
tasksToRecreate{3}.name  = 'PassByRight2blocks';

% --------------------------------
% Task: flickering stimulus
flickerQ                 = polGtaskGQ(:,:,:,:,1);
flickerQ(:,:,1:2:end,:)  = polTtaskTQ(:,:,1:2:end,:,1); % polGtaskTQ(:,:,1:2:end,:,1);
tasksToRecreate{4}.allQtoRecreate = flickerQ; %flickerQ(:,:,:,:,1);
tasksToRecreate{4}.name     = 'FlickerStim';


% -------------------------------------------------------------------------
% -------------------------------------------------------------------------
% Find optimal reconstruction of novel task Q-values

% all Threat Presences to test the extents with. 0 is just goal, 1 is just
% 'threat' (which could also have positive valence)
aTP = [0 1];

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Calculate neural activities
squishAct = nan([14 15 7 18 2 size(rSall,[1 2])]);
tic
for iM = 1:size(allNetAR,1)
    for iRun = 1:size(allNetAR,2)
        for iTyp = 1:size(allNetAR,3)
            for iReg = 1:size(allNetAR,4)
                for iV = 1:2 %length(aTP)

                    sFP = DefaultSettings(rSall(iM,iRun,iTyp,iReg).s);
                    sFP.plt.otherStateVars = 3;
                    w = rSall(iM,iRun,iTyp,iReg).w;
                    net = rSall(iM,iRun,iTyp,iReg).net;

                    sFP.plt.plotThrFl = aTP(iV);
                    sFP.plt.rowLims = [1.5 sFP.wrld.size(1)-0.5];
                    sFP.plt.stimRow=[3:size(w.world2D,1)-3];
                    sFP.plt.meanLimbCols = 1;
                    sFP.plt.fitSigmoid = 0;
                    sFP.plt.lmbCol = [2:(sFP.wrld.size(2)-1)];
                    sFP.plt.stimCol= [2:(sFP.wrld.size(2)-1)] ;
                    [Q,allNeurAct] = CalcNetOutput(sFP,w,net);

                    sFP.plt.plAct = 1;
                    sFP.plt.ON = 0;
                    for iL = 1:size(allNeurAct,5)
                        for iN = 1:size(allNeurAct,6)
                            [dmy, squishAct(:,:,iL,iN,iV,iM,iRun,iTyp,iReg)] = DisplActValsFun(sFP,w,permute(allNeurAct(:,:,:,:,iL,iN), [3 4 1 2 5 6]));
                        end
                    end
                end
            end
        end
    end
end
toc

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Calculate neural selectivity
clear neurSelectivity allNS
allNS = nan([7 18 3 15 7 2]);

for iM = 1:size(allNetAR,1)
    for iRun = 1:size(allNetAR,2)
        for iTyp = 1:size(allNetAR,3)
            for iReg = 1:size(allNetAR,4)
                % Divide the neurons up in different ways
                % % (e.g. can only takie the 2nd half of the network:
                % inclLays = ceil(size(allNeurAct,5)./2) : size(allNeurAct,5);
                inclLays = [1 2 3 4];

                % General selectivity rankings
                tmpNeurs            = allNetAR(iM,iRun).A_B_rat(inclLays,:);
                toSort              = abs(tmpNeurs - 0.5);
                [dmy, sortInds]     = sort(toSort(:));
                [dmy, selectRanks]  = sort(sortInds);
                selectRanks         = reshape(selectRanks,size(tmpNeurs));

                % Goal selectivity rankings
                toSort            = tmpNeurs;
                [dmy, sortInds]   = sort(toSort(:));
                [dmy, goalRanks]  = sort(sortInds);
                goalRanks         = reshape(goalRanks,size(tmpNeurs));

                % Threat selectivity rankings
                toSort             = 1 - tmpNeurs;
                [dmy, sortInds]    = sort(toSort(:));
                [dmy, threatRanks] = sort(sortInds);
                threatRanks        = reshape(threatRanks,size(tmpNeurs));

                % Keep track of how many entries are Not a Neuron (ha)
                nanNums = sum(~isnan(tmpNeurs(:))) + 1;

                % 2nd half vs 1st half
                neurSelectivity                               = selectRanks;
                neurSelectivity(1:floor(end/2),:)             = 0;
                neurSelectivity(floor(end/2) + 1 : end,:)           = 1;
                % % %         neurSelectivity(1 : end,:)           = 1;
                neurSelectivity(selectRanks >  nanNums)       = NaN;

                % Store neural selectivity
                allNS(inclLays,1:size(neurSelectivity,2),iM,iRun,iTyp,iReg,:) = neurSelectivity;
            end
        end
    end
end
% Replace the the non-selected layers of the net with NaNs
allNS(1:min(inclLays)-1,:,:,:,:) = NaN;



% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Fit net activity to alternative tasks, and calculate best correlation
% coefficient: rhoAll

clear rho pp rhoAll
% iSelect,iM,iRun,iTyp,iReg
rho = nan([2 3 17 7 2]);

for iFig = 1:4
allQtoRecreate  = tasksToRecreate{iFig}.allQtoRecreate;
% Only recreate Q for 1 handpos (to keep the size small)
allQtoRecreateForNeurs = allQtoRecreate(1,8,:,:,1);
% Searately use un-selective neurons (0) and selective neurons (1)
allSelect = [0 1];
tic
for iM = 1:size(allNetAR,1)
    for iRun = 1:size(allNetAR,2)
        for iTyp = 1:size(allNetAR,3)
            for iReg = 1:size(allNetAR,4)
                % Set selectivity: unselective (0) or selective (1)
                for iSelect = 1:numel(allSelect)
                    cSelect = allSelect(iSelect);
                    clear baseQNeurs

                    % Convert to baseQ
                    % Loop through pseudo-policies
                    for iPol    = 1:2
                        if size(allNS,7) == 1
                            inclNeurs = permute(allNS(:,:,iM,iRun,iTyp,iReg), [3 4 1 2]);
                            inclNeurs = inclNeurs(:) == cSelect;
                            % This is in case there are non-mutually-exclusive neuron
                            % groupings
                        else
                            inclNeurs = permute(allNS(:,:,iM,iRun,iTyp,iReg,iSelect), [3 4 1 2]);
                            inclNeurs = inclNeurs(:) == 1;
                        end
                        tmpNeurAct = squishAct(:,:,:,:,iPol,iM,iRun,iTyp,iReg);
                        % row col row col act pol task [NOTE: act will just be 1, so the
                        % neurons take the role of tasks in the formal theory]
                        baseQNeurs(:,:,:,:,1,iPol,:) = permute(tmpNeurAct(:,:,inclNeurs),[4 5 1 2 3]);
                    end

                    % ============================
                    % Fit the ideal Q-values using neural activities as feature-components
                    % Define function to be optimised
                    FunToOpt = @(p) ErrFun(baseQNeurs, allQtoRecreateForNeurs, p, cAct, sFP);

                    % Set optimisation options - keep it simple
                    A = []; b = []; Aeq = []; beq = [];
                    w0 = ones([size(baseQNeurs,7), 1]);
                    lb = -[Inf Inf]';
                    ub = [Inf Inf]';
                    % Run optimisation
                    OPTIONS = optimset('TolCon',1e-10);
                    [p,sSqErr,exitflag,output,lambda,grad,hessian] = fmincon(FunToOpt,w0,A,b,Aeq,beq,lb,ub,[],OPTIONS);

                    % Extract optimised data
                    [sSqErrFinal, bestPsi] = ErrFun(baseQNeurs, allQtoRecreateForNeurs, p, cAct, sFP);

                    fittedData   = zeros(size(baseQNeurs(:,:,:,:,1)));
                    weightedData = nansum(baseQNeurs(:,:,:,:,cAct,:,:,:) .* permute(p,[2 3 4 5 6 7 1]),7);
                    for r = 1:size(bestPsi,1)
                        for c = 1:size(bestPsi,2)
                            for rr = 1:size(bestPsi,3)
                                for cc = 1:size(bestPsi,4)

                                    % Select the correct policy for each condition
                                    fittedData(r,c,rr,cc) = weightedData(r,c,rr,cc,bestPsi(r,c,rr,cc));

                                end
                            end
                        end
                    end
                    [rho(iSelect,iM,iRun,iTyp,iReg) pp(iSelect,iM,iRun,iTyp,iReg)] = corr(allQtoRecreateForNeurs(:),fittedData(:));
                end
            end
        end
    end
end
rhoAll(:,:,:,:,:,iFig) = rho;
toc
end
% Sanity check in case more networks accidentally added
rhoAll = rhoAll(:,:,1:15,:,:,:);

% Only keep the reconstructions made using selective neurons 2), or the 
% unselective neurons (1), or all neurons (:)
% rhoAllNewTasks = squeeze(rhoAll(1,:,:));
rhoAllNewTasks = squeeze(rhoAll(2,:,:));
% rhoAllNewTasks = nanmean(rhoAll(:,:,:),1);



% -------------------------------------------------------------------------
% Save the processed data
svNm = ['Results\ForFigures\Fig5_ExtDatFig3_model6_Base_Processed_' netSize '_Precomputed.mat' ];
save(svNm, 'pNodeDists', 'tStatsStructure', 'alldotProdBs', 'alldotProd', 'alldotProdPm' , ...
     'rhoAllNewTasks', 'rSall', 'allPerf', 'allNetAR' , ...
     'neurTypes', 'regTypes', '-v7.3');

%% EMPIRICAL DATA, PART 1/2: Generating modelled data (Fig 6,7, ExtDat Fig 4,5)

CreateModelDataToFit_Final;

% Save modelled data 
save('Results\ForFigures\Fig6_7_ModelledData_Precomputed.mat', ...
'sBdy', 'sHnd', 'sHndSide', 'sHed', 'sTrack', 'sArm', 'sBack', 'sHndToolRake', 'd', 'dQ', 'dUn', 'dSr', 'dHP', 'dMI', 'dDi', 'dNegDi', 'fitRes', 'allQ', '-v7.3');


%% EMPIRICAL DATA, PART 2/2: Fitting empirical data to generated 
%  modelled data (Fig 6,7, ExtDat Fig 4,5)

FitEmpiricalData_Final;

% Save data fitting results
save('Results\ForFigures\Fig6_7_FittedData_Precomputed.mat', ...
 'fitRes','dQ','dUn','dSr','dHP','dMI','dDi','dNegDi','k', '-v7.3');


%% ANN FOR EXTENDED DATA FIGURE 1: Reward only after 'eating' (ExtDat Fig 1)

clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% Base settings
s.gol.alSpC = [0 0];
s.gol.randSpr = [0 0];
s.lp.b1Siz = 1e4;
s.lp.minExp = 4e7; % minumum number of actions before training sarts (big here so no training)
s.lp.dispA = 1e6; % only show action count very infreequently
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.rp.maxActions = 4e6 ;
s.lmb.startCol=8;
s.fl.trainTable=1; s.fl.trainNet=1;
s.rl.maxRetr= 100;

netSizes={[16 14 12 10 8 6]};

% ---------------------------
% Unique settings
s.fl.grabAndEat = 1;
s.act.eatRew = 1;
s.act.GoalRew = 0; 

s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=10;

s.lp.gamma = 0.8;

% -------------------------------------------------------------------------
% Run the model
[s w storedEps net Qtable] = RunRLfun(s);
% -------------------------------------------------------------------------
% Learn the Q-values
for iM = 1:length(netSizes)
    
    s.fl.newNet = 1;
    s.fl.newTable = 1;
    
    % Change specific settings of the model
    s.lp.netS = netSizes{iM};

    % Relearn Q with different netsizes
    [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable);
    % Store network results Structure
    ntRS(iM).s=s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
    ntRS(iM).perf = perf;
    
end

bFld = 'Results\ForFigures\';
svNm = [bFld 'ExtDatFig1_EatingModel.mat'];
save(svNm,'ntRS','-v7.3');

%% ANNs for other learning algorithms, e.g. SARSA, on-policy learning (Fig S1)

% -------------------------------------------------------------------------
% Base settings
s = DefaultSettings();
s.gol.alSpR = 1;
s.gol.alSpC = [0 0];
s.gol.randSpr = [2 2];
s.lp.b1Siz = 1e4;
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol = 8;
s.fl.trainTable = 1; s.fl.trainNet=1;
s.rl.maxRetr = 20;
s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches = 5;

netSizes={[12 10 8 6],[9 9 9 9],[6 8 10 12]};

% --------------------------------------
% Unique settings
s.lp.epsilon = 0.8;
% Policy: unbalanced-epsilon-greedy
unbPi = @(optimal_action, n_actions, randNum) (randNum > s.lp.epsilon) * optimal_action + (randNum <= s.lp.epsilon) * 3;

allGoalRewards = [-2 2] ;

% Create threat stored epochs
storedEpsThr = storedEps;
storedEpsThr.R(storedEps.R > 0.1) = - storedEpsThr.R(storedEps.R > 0.1);

for iRew = 1:length(allGoalRewards)

s.act.GoalRew = allGoalRewards(iRew);


for iDepth = 1:3
    % Run each type of model

    if iDepth == 1
        % Run the model
        [s w storedEps net Qtable] = RunRLfun(s);
    end

    for iM=1:length(netSizes)

        if iDepth == 1
            s.fl.newNet = 1;
            s.fl.newTable = 1;
    
            % Make sure the correct stored epochs are used
            if allGoalRewards(iRew) < 0
                storedEpsSARSA{iDepth}      = storedEpsThr;
                storedEpsSARSAUnb{iDepth}   = storedEpsThr;
                storedEpsQ{iDepth}          = storedEpsThr;
            else
                storedEpsSARSA{iDepth}      = storedEps;
                storedEpsSARSAUnb{iDepth}   = storedEps;
                storedEpsQ{iDepth}          = storedEps;
            end

            netSARSA    = net;
            netSARSAUnb = net;
            netQ        = net;
        else
            % Set parameters for nth run, using the network to make decisions
            s.fl.newNet = 0;
            s.fl.newTable = 1;

            s.fl.newNet     = 0;
            s.lp.retr       = 1000000;
            s.rp.maxActions =  100000;
            % Change specific settings of the model
            s.lp.netS = netSizes{iM};

            s.lp.alg = 'SARSA';
            [s w storedEpsSARSA{iDepth} netSARSA Qtable] = RunRLfun(s,rSsarsa(iM,iDepth-1).net,[]);
                        
            sUnb = s;
            sUnb.act.pi = unbPi;
            [sUnb w storedEpsSARSAUnb{iDepth} netSARSAUnb Qtable] = RunRLfun(sUnb,rSsarsaUnb(iM,iDepth-1).net,[]);

            s.lp.alg = 'Q';
            [s w storedEpsQ{iDepth} netQ Qtable] = RunRLfun(s,rS(iM,iDepth-1).net,[]);
            
            netSARSA    = rSsarsa(iM,iDepth-1).net;
            netSARSAUnb = rSsarsaUnb(iM,iDepth-1).net;
            netQ        = rS(iM,iDepth-1).net;
        end

        % Change specific settings of the model
        s.lp.netS = netSizes{iM};

        s.lp.alg = 'SARSA';
        [net Qtable perf] = RelearnNNFun(s, w, storedEpsSARSA{iDepth}, ...
            netSARSA, Qtable);
        % Store results Structure: SARSA
        rSsarsa(iM,iDepth).s=s; rSsarsa(iM,iDepth).Qtable=Qtable;
        rSsarsa(iM,iDepth).w=w; rSsarsa(iM,iDepth).net=net;
        rSsarsa(iM,iDepth).perf=perf;

        sUnb = s;
        sUnb.act.pi = unbPi;
        [net Qtable perf] = RelearnNNFun(sUnb, w, storedEpsSARSAUnb{iDepth}, ...
            netSARSAUnb, Qtable);
        % Store results Structure: SARSA but with UNBALANCED epsilon policy
        rSsarsaUnb(iM,iDepth).s=s; rSsarsaUnb(iM,iDepth).Qtable=Qtable;
        rSsarsaUnb(iM,iDepth).w=w; rSsarsaUnb(iM,iDepth).net=net;
        rSsarsaUnb(iM,iDepth).perf=perf;

        s.lp.alg = 'Q';
        [net Qtable perf] = RelearnNNFun(s, w, storedEpsQ{iDepth}, ...
            netQ, Qtable);
        % Store results Structure: Q-learning
        rS(iM,iDepth).s=s; rS(iM,iDepth).Qtable=Qtable;
        rS(iM,iDepth).w=w; rS(iM,iDepth).net=net;
        rS(iM,iDepth).perf=perf;
    end

    if allGoalRewards(iRew) < 0
        rSsarsathr      = rSsarsa; 
        rSsarsaUnbthr   = rSsarsaUnb;
        rSthr           = rS;
    end
    
    svNm='Results\ForFigures\FigS1_Algorithm_Effects.mat';
    save(svNm,'rSsarsa','rSsarsaUnb','rS','rSsarsathr','rSsarsaUnbthr','rSthr','-v7.3')

end

end


%% Tabular claculation of Q-values for the 'wasp encounter' scenario (Fig S2)

s.wrld.size = [61 61 31];
s.clc.nearPos = [s.wrld.size(1)-6 ceil(s.wrld.size(2)./2) ceil(s.wrld.size(3)./2)]';
s.clc.nReps = 1;
s.clc.gammaVal   = 0.8;
s.clc.baseVel    = [4 0 0]; 

allQ = table();

% Random stimulus dynamics
rSpr = [-2 -1 0 1 2]; % row Spread
rSprPr = gaussmf(rSpr,[1 0]) ./ sum(gaussmf(rSpr,[1 0])); % row Spread prob
cSpr = [-2 -1 0 1 2];
cSprPr = gaussmf(cSpr,[1 0]) ./ sum(gaussmf(cSpr,[1 0]));
zSpr = [0];
zSprPr = gaussmf(zSpr,[1 0]) ./ sum(gaussmf(zSpr,[1 0]));

% x y z, Deterministic stimulus dynamics
s.clc.stimDynams =     @(pos) pos + s.clc.baseVel; % For approaching, set speed positive
s.clc.randSpread =     {rSpr cSpr zSpr}; 
s.clc.spreadProb =     {rSprPr cSprPr  zSprPr}; % x y z, probabilities of spread

% FIRST CALCULATE FOR BODY - moves more slowly than limb. Also
% ONLY has negative potential rewards
sBdy = s;
sBdy.clc.actConsequence = [ 0  0  0 ; ... % action 1 stay
    0  1  0 ; ... % action 2：body left
    0 -1  0 ; ... % action 3: body right
    0  0 -1 ; ... % action 4: hand left
    0  0  1];     % action 5: hand right

% Location and size of limb, and where the Q-values are calculated FROM
sBdy.clc.startSR = []; sBdy.clc.startSC = []; sBdy.clc.startSZ = [];

bdyRs = s.clc.nearPos(1) + [2 1 1 0 0 0 1 1 2];
bdyCs = s.clc.nearPos(2) + [-4:4];
bdyZs = s.clc.nearPos(3) + [-floor(s.wrld.size(3)./2):floor(s.wrld.size(3)./2)];
iRew = 0; % Initialise rewarded block counter
sBdy.clc.rewSplitInd(1) = 1;
for  iZ = 1:length(bdyZs)
    for  iC = 1:length(bdyCs)
        iRew = iRew + 1;
        sBdy.clc.startSR(iRew) =  bdyRs(iC);
        sBdy.clc.startSC(iRew) =  bdyCs(iC);
        sBdy.clc.startSZ(iRew) =  bdyZs(iZ);
    end
end
sBdy.clc.rewSplitInd(2) = iRew + 1;

% Add in sliding 'hand'
baseDisplace = s.clc.nearPos(2) - ceil(length(bdyZs)./2);
sBdy.handWidth = 5;
for  iZ = 1:length(bdyZs)
    handCenter = baseDisplace + (iZ - 1);

        cRew = iRew + [1:sBdy.handWidth];
        sBdy.clc.startSR(cRew) =  min(bdyRs) - 1; % Hand should be in front of body
        sBdy.clc.startSC(cRew) =  handCenter + [1:sBdy.handWidth] - floor(sBdy.handWidth./2);
        sBdy.clc.startSZ(cRew) =  bdyZs(iZ);
        iRew = max(cRew);
end
sBdy.clc.rewSplitInd(3) = iRew + 1;

% Run the model
% -----------------------------
% DON't allow the body to move directly away, show it a FAST wasp
cQ = 1;
sBdy.clc.startRew = [-2 -1] ; % Body, then limb
[newQ ] = CalcQDirect(sBdy);
% store q values and attributes
allQ.qVals{cQ}       = newQ;
allQ.dir(cQ)         = 1; %s.clc.stimDynams([0 0 0]); % towards
allQ.rew(cQ,:)       = sBdy.clc.startRew; % towards
allQ.bodyPart{cQ}    = 'No Body Move, Fast Wasp'; 
allQ.centerpos{cQ}   = sBdy.clc.nearPos; % x y z position of this plot in the OVERALL space --> trunk is central


% -----------------------------
% Second sim: allow the body to move directly away, show it a FAST wasp

% (NOTE: If the limb can move at different speeds, BOTH speeds have to be
% put in as potential actions, because the model doesn't have any collision
% calculation that takes into account overshoot)

cQ = 2;
sBdyAWAY = sBdy;
sBdyAWAY.clc.actConsequence = [ 0  0  0 ; ... % action 1 stay
    0  1  0 ; ... % action 2：body left
    0 -1  0 ; ... % action 3: body right
    0  0 -1 ; ... % action 4: hand $left$
    0  0  1 ; ... % action 5: hand $right$
    -2  0  0];     % action 6: Body AWAY


[newQ ] = CalcQDirect(sBdyAWAY);
% store q values and attributes
allQ.qVals{cQ}       = newQ;
allQ.dir(cQ)         = 1; % towards
allQ.rew(cQ,:)       = sBdyAWAY.clc.startRew; % towards
allQ.bodyPart{cQ}    = 'No Body Move, Fast Wasp'; 
allQ.centerpos{cQ}   = sBdyAWAY.clc.nearPos; % x y z position of this plot in the OVERALL space --> trunk is central


% -----------------------------
% Third sim: allow the body to move directly away, show it a SLOW wasp
cQ = 3;
sBdyAWAYslowwasp = sBdyAWAY;
% Set slow wasp move speed
sBdyAWAYslowwasp.clc.baseVel    = [1 0 0]; 
sBdyAWAYslowwasp.clc.stimDynams =     @(pos) pos + sBdyAWAYslowwasp.clc.baseVel;
[newQ ] = CalcQDirect(sBdyAWAYslowwasp);
% store q values and attributes
allQ.qVals{cQ}       = newQ;
allQ.dir(cQ)         = 1; ; % towards
allQ.rew(cQ,:)       = sBdyAWAYslowwasp.clc.startRew; % towards
allQ.bodyPart{cQ}    = 'No Body Move, Fast Wasp'; 
allQ.centerpos{cQ}   = sBdyAWAYslowwasp.clc.nearPos; % x y z position of this plot in the OVERALL space --> trunk is central

save('Results\FigS2_WaspMovement.mat',...
    'allQ','sBdy','-v7.3')


%% Learning effects model: recreating Coello et al 2018

load('Results\ForFigures\Fig6_7_ModelledData_Precomputed.mat')

% -------------------------------------------------------------------------
% Find the values of hand- actions at the 'height of the table', using the 
% Qs averaged from approaching and retreating
qOnTable = allQ(1,:).qVals{1}(:,:,:,2);

% Define the positions of the real stimuli
% Assume hand is just at the base of the central block
blockXsize = 88.56/7;
blockYsize = 49.81/6;
stimPosX   = blockXsize .* -4   + blockXsize .* (1:7);
stimPosY   = blockYsize .* -0.5 + blockYsize .* (1:6); 
[sX, sY]    = meshgrid(stimPosX,stimPosY);

onTableQPosY = 62 - (1:size(qOnTable,2));
onTableQPosY = 5.* (  onTableQPosY - ((size(qOnTable,2)+1) - allQ(1,:).centerpos{1}(1))  );
onTableQPosX = 5.* (  (1:size(qOnTable,3)) - allQ(1,:).centerpos{1}(2)  );
[qX qY]      = meshgrid(onTableQPosX,onTableQPosY);

figure,imagesc(qX(:),qY(:),squeeze(max(qOnTable)) );
hold on
scatter(sX,sY,20,[0 0 0],'filled');
axis xy

% Find action values at positions in question
clear stimPosQ
for iSX = 1:numel(stimPosX)
    xBin = findnearest(stimPosX(iSX), onTableQPosX );
    for iSY = 1:numel(stimPosY)
        yBin = findnearest(stimPosY(iSY), onTableQPosY );
        stimPosQ(:,iSY,iSX) = qOnTable(:,yBin,xBin);
    end    
end



% -------------------------------------------------------------------------
% Calculate BASE probability of performing reaching actions

sHnd.clc.baseVel = [0 0 0];

sHnd.rch.temp = 0.07;
sHnd.rch.nTP  =  20;
% ===========================================================
% CASE: 50-50 chance of reward everywhere (just a flat line basically)
sHnd.rch.farVal = 1.0;
sHnd.rch.nearVal = 1.0;
sHnd.rch.tableXMin = -88.56/2 ;
sHnd.rch.tableXMax =  88.56/2 ;
sHnd.rch.tableYMin =   8      ; % Width of half of hand
sHnd.rch.tableYMax =  49.81   ;

[handLocProbs, transProbs, onTableQPosX, onTableQPosY] = CalcReachProbabilities(sHnd,allQ);

figure,imagesc(onTableQPosX, onTableQPosY, handLocProbs(:,:,sHnd.rch.nTP)); axis xy
xlim([-50 50]);
ylim([0 100]);
SquareAxes;

% ===========================================================
% CASE: 75% chance of reward far, 25% of reward near
sHnd.rch.farVal  = 1.50;
sHnd.rch.nearVal = 0.75;

[handLocProbs_FarRew, transProbs_FarRew] = CalcReachProbabilities(sHnd,allQ);

figure,imagesc(onTableQPosX, onTableQPosY, handLocProbs_FarRew(:,:,sHnd.rch.nTP)); axis xy
xlim([-50 50]);
ylim([0 100]);
SquareAxes;

% ===========================================================
% CASE: 25% chance of reward far, 75% of reward near
sHnd.rch.farVal  = 0.75;
sHnd.rch.nearVal = 1.50;

[handLocProbs_NearRew, transProbs_NearRew] = CalcReachProbabilities(sHnd,allQ);

figure,imagesc(onTableQPosX, onTableQPosY, handLocProbs_NearRew(:,:,sHnd.rch.nTP)); axis xy
xlim([-50 50]);
ylim([0 100]);
SquareAxes;

% -------------------------------------------------------------------------
% Calculate RELEARNED reaching action proababilities for a variety of 
% different learning rates, temperatures, and blocks

s2.nSteps           = 39;
s2.initV            = [1 1]';
s2.alpha            = 0.4;
s2.rew              = [1.5 0.75]';
s2.gamma            = 0;
valsForReach        = applyTDLearn(s2); % Familiarisation

% Make a plot to test what kind of alpha we need
figure,plot([1, 1 ; valsForReach'])

allAlphas = [0.1:0.1:0.9 , 0.07 0.05 0.03];

allTemps = [ 0.05 0.07 0.1 0.14 0.2 0.3];

for iTemp = 1:numel(allTemps)

    sHnd.rch.temp = allTemps(iTemp);

    for iAlpha = 1:numel(allAlphas)

        % Set learning rate Alpha
        s2.alpha = allAlphas(iAlpha);

        % Update values of states as a function of time spent leanring
        valsForReach       = applyTDLearn(s2);

        % Insert the 'no learning yet' stage
        valsForReach       = [s2.initV, valsForReach];

        for iLearnTP = 1:40

            sHnd.rch.nTP  = 20;
            % ==================================================
            % CASE: 50-50 chance of reward everywhere (just a flat line basically)
            sHnd.rch.farVal = 1.0;
            sHnd.rch.nearVal = 1.0;

            sHnd.rch.tableXMin = -88.56/2 ;
            sHnd.rch.tableXMax =  88.56/2 ;
            sHnd.rch.tableYMin =   8      ; % Width of half of hand
            sHnd.rch.tableYMax =  49.81   ;

            if iAlpha == 1 && iLearnTP == 1
                % This only needs to happen once because learning changes ABOLUTELY NOTHING
                [handLocProbs_Learn(:,:,:,iLearnTP,iAlpha,iTemp), ~, onTableQPosX, onTableQPosY] = CalcReachProbabilities(sHnd,allQ);
            else
                handLocProbs_Learn(:,:,:,iLearnTP,iAlpha,iTemp) = handLocProbs_Learn(:,:,:,1,1,iTemp);
            end

            % ==================================================
            % CASE: 75% chance of reward far, 25% of reward near
            sHnd.rch.farVal  = valsForReach(1,iLearnTP);
            sHnd.rch.nearVal = valsForReach(2,iLearnTP);

            [handLocProbs_Learn_FarRew(:,:,:,iLearnTP,iAlpha,iTemp)] = CalcReachProbabilities(sHnd,allQ);


            % ==================================================
            % CASE: 25% chance of reward far, 75% of reward near
            sHnd.rch.farVal  = valsForReach(2,iLearnTP);
            sHnd.rch.nearVal = valsForReach(1,iLearnTP);

            [handLocProbs_Learn_NearRew(:,:,:,iLearnTP,iAlpha,iTemp)] = CalcReachProbabilities(sHnd,allQ);

            disp(['alpha: '   num2str(iAlpha)]);
            disp(['learnTP: ' num2str(iLearnTP)]);
        end
    end
    toc
end


% -------------------------------------------------------------------------
% Make sure the Control values are properly filled out
for iTemp = 1:6

    sHnd.rch.temp = allTemps(iTemp);
    sHnd.rch.nTP  =  20;
    % ==========================================================
    % CASE: 50-50 chance of reward everywhere (just a flat line basically)
    sHnd.rch.farVal = 1.0;
    sHnd.rch.nearVal = 1.0;


    sHnd.rch.tableXMin = -88.56/2 ;
    sHnd.rch.tableXMax =  88.56/2 ;
    sHnd.rch.tableYMin =   8      ; % Width of half of hand
    sHnd.rch.tableYMax =  49.81   ;

    [handLocProbs, transProbs, onTableQPosX, onTableQPosY] = CalcReachProbabilities(sHnd,allQ);

    handLocProbs_Learn(:,:,:,:,:,iTemp) =  repmat(handLocProbs,[1 1 1 size(handLocProbs_Learn_FarRew,4) size(handLocProbs_Learn_FarRew,5)]);
end

% -------------------------------------------------------------------------
% Reproduce the findings from the Coello 2018 paper:
%  Find the optimal parameters

sHnd.rch.fitReachLinearly          = 0;
sHnd.rch.renormaliseOnTableSurface = 1;
sHnd.rch.plotFl     =    0;
sHnd.rch.cDiffuseTP =   20;
sHnd.rch.cAlpha     =    4;
sHnd.rch.cTemp      =    2;
sHnd.rch.cLearnTP   = 1:40; 
sHnd.rch.gaussKern  =    6; 
sHnd.rch.gaussSigm  =    1;


sHnd.rch.nFreeParams = 0;

% Loop through the fitting parameters
nGaussSigma = 5;
nDifTP = 21;
for iGaussSigm = 1:nGaussSigma

    sHnd.rch.gaussSigm  = iGaussSigm - 1;
    % ------------------------------------------------
    % Make gaussian blurred version
    handLocProbs_Learn_G         = GaussBlur(handLocProbs_Learn,        sHnd.rch.gaussKern,sHnd.rch.gaussSigm );
    handLocProbs_Learn_FarRew_G  = GaussBlur(handLocProbs_Learn_FarRew, sHnd.rch.gaussKern,sHnd.rch.gaussSigm );
    handLocProbs_Learn_NearRew_G = GaussBlur(handLocProbs_Learn_NearRew,sHnd.rch.gaussKern,sHnd.rch.gaussSigm );

    for iAlpha = 1:size(handLocProbs_Learn_FarRew,5)
        for iDiffuseTP = 1:21
            for iTemp  = 1:size(handLocProbs_Learn_FarRew,6)

                sHnd.rch.cAlpha     = iAlpha ;
                sHnd.rch.cDiffuseTP = iDiffuseTP;
                sHnd.rch.cTemp      = iTemp;

                [fitRes, normSSqErrAll(iAlpha,iDiffuseTP, iTemp, iGaussSigm), realDat, realDatStd, modelDat] = ...
                    ReproduceReachingExperiment(sHnd,...
                    handLocProbs_Learn_G,handLocProbs_Learn_FarRew_G,handLocProbs_Learn_NearRew_G,...
                    qX,qY,onTableQPosX,onTableQPosY);

                gofScoreAll(iAlpha,iDiffuseTP, iTemp, iGaussSigm) = fitRes.gofScore;
                pValAll(iAlpha,iDiffuseTP, iTemp, iGaussSigm)     = fitRes.pVal1;
            end
        end
        
    end

    disp(['Tested blur levels ' num2str(iGaussSigm) '/' num2str(nGaussSigma) ] );
    disp(['Tested alpha ' num2str(iAlpha) '/' num2str(size(handLocProbs_Learn_FarRew,5)) ] );
    disp(['Tested diffusion TP ' num2str(iDiffuseTP) '/' num2str(nDifTP) ] );
    disp(['Tested temperature ' num2str(iTemp) '/' num2str(size(handLocProbs_Learn_FarRew,6)) ] );
    
end


% -------------------------------------------------------------------------
% Store optimal parameters

normSSqErrAll(normSSqErrAll==0) = NaN;
sHnd.rch.svPlFl                    = 1;
sHnd.rch.fitReachLinearly          = 1;
sHnd.rch.renormaliseOnTableSurface = 1;

[dmyMin, minInd] = min(normSSqErrAll,[],'all')
[dmyMin, minInd] = max(pValAll,[],'all')

[rr, cc, hh1, hh2] = ind2sub(size(pValAll), minInd)

sHnd.rch.plotFl     =   1;
sHnd.rch.cAlpha     = rr;
sHnd.rch.cDiffuseTP = cc;
sHnd.rch.cTemp      = hh1;
sHnd.rch.gaussSigm  = hh2 - 1; % Because there's also zero gaussian

% ------------------------------------------------
% Add Gaussian blur
handLocProbs_Learn_G         = GaussBlur(handLocProbs_Learn,        sHnd.rch.gaussKern,sHnd.rch.gaussSigm );
handLocProbs_Learn_FarRew_G  = GaussBlur(handLocProbs_Learn_FarRew, sHnd.rch.gaussKern,sHnd.rch.gaussSigm );
handLocProbs_Learn_NearRew_G = GaussBlur(handLocProbs_Learn_NearRew,sHnd.rch.gaussKern,sHnd.rch.gaussSigm );

% ------------------------------------------------
% Save relevant variables for plotting
save('Results\ForFigures\FigS5_Coello.mat', ...
    'sHnd','handLocProbs_Learn_G','handLocProbs_Learn_FarRew_G','handLocProbs_Learn_NearRew_G', ...
    'qX','qY','onTableQPosX','onTableQPosY');





%% ANN Model 7, PART 1/2: running models (Fig S3)
%  Data for plot S3 about simulated reaction times and TMS, 
%  and the corresponding stats

clear s rS ntRS rtRS ytRS olRS

% -------------------------------------------------------------------------
% First load bodypart-centred Q-values
load('ForFigures\Fig2abc_model1_1Limb.mat');
iM = 2;

s = rS(iM).s;
w = rS(iM).w;
net = rS(iM).net;

s.plt.rowLims=[1.5 s.wrld.size(1)-1.5];
s.plt.colLims=[1.5 s.wrld.size(2)-2.5];

s.plt.lmbCol = 8;

[QPPS,allNeurAct] = CalcNetOutput(s,w,net);

DisplActValsFun(s,w,QPPS);

caxis([0 2]);
colormap(whitetocol(100,[0 0 0.7]));

% -------------------------------------------------------------------------
% Then run the reaction time scenario
clear rS storedEps;

s.lp.netS = [9 9];

s.fl.newNet = 1;
s.fl.newTable = 1;

s.rp.maxActions = 2e5 ;
s.lp.minExp = 2e6 ;

s.fl.rtTask=1;
s.fl.hist=0;
s.act.Name={'LEFT','STAY','RIGHT','BUTTON'};
s.fl.dspm=0;


s.act.rtRew=1;
s.rtt.threshold=3;

for defaultRew = [0, -0.01]
    
    clear rS
    
    s.act.BumpRew = defaultRew;
    s.act.GoalRew = defaultRew;
    s.act.ThreatRew = defaultRew;
    s.act.DefaultMoveRew = defaultRew;
    s.act.bdyBumpRew = defaultRew;
    s.act.bdyGoalRew = defaultRew;
    s.act.bdyThreatRew = defaultRew;
    s.act.bdyMoveRew = defaultRew;
    
    s=DefaultSettings(s);
    
    
    s.rl.maxRetr=100;
    
    extraInputs = [0 1];
    
    
    s.fl.perfWhileLearn = 1;
    s.prf.nRep = 10;
    s.prf.skipBatches = 49;
    
    % Run each type of model
    for iM = 1:length(extraInputs)
        
        s.fl.newNet = 1;
        s.fl.newTable = 1;
        
        % Change specific settings of the model
        s.fl.extraInput = extraInputs(iM);
        
        % Run model with new settings (extra Q-inputs)
        [s w storedEps net Qtable]=RunRLfun(s,[],[],[],QPPS);
        
        % Relearn Q with different netsizes
        [net Qtable perf] = RelearnNNFun(s,w,storedEps,net,Qtable,QPPS);
        % Store network results Structure
        rtRS(iM).s=s; rtRS(iM).Qtable=Qtable; rtRS(iM).w=w; rtRS(iM).net=net;
        rtRS(iM).perf = perf;
        
        bFld = 'Results\ForFigures\';
        svNm = [bFld 'FigS3ab_model7_RTtask_BaseRew_' num2str(defaultRew) '.mat'];
        save(svNm,'rtRS','QPPS','-v7.3');
        
    end
end

%% ANN Model 7, PART 2/2: performing analyses (Fig S3)

load('FigS3ab_model7_RTtask_BaseRew_-0.01.mat');

% number of repetitions of simulated reaction time task.
nRep = 50;

clear Q

for iM=1:length(rtRS)+1
    
    % For iM == 3, use QPPS but full of the median QPPS output
    if iM < 3
        cM = iM;
        QPPS2 = QPPS;
    else 
        cM = 2;
        QPPS2(:,:,:,:,:) = 0;nanmedian(QPPS(:));
    end
    
    
    sFP = DefaultSettings(rtRS(cM).s); 
    w = rtRS(cM).w;
    net = rtRS(cM).net;
    Qtable = rtRS(cM).Qtable;
    
    % settings for plot
    sFP.plt.lmbCol = 3:sFP.wrld.size(2)-2;
    sFP.plt.ON = 0;
    sFP.plt.sequentialLimbCols = 0;
    sFP.plt.stimRow = [3:size(w.world2D,1)-1];
    sFP.plt.stimCol = [2:size(w.world2D,2)-1];
    sFP.plt.pltType = 'Binned';
    
    sFP.plt.rtTouchVal = sFP.rtt.threshold;
    
    for iRep = 1:nRep
        sFP.plt.rtTouchVal = sFP.rtt.threshold+sFP.rtt.NoiseFun(sFP.rtt.mu,sFP.rtt.std);
        touchVals(iRep) = sFP.plt.rtTouchVal;
        [Q(:,:,:,:,:,iRep),allNeurAct] = CalcNetOutput(sFP,w,net,QPPS2);
    end
    
    
    qDiff = squeeze(Q(:,:,:,:,4,:)-Q(:,:,:,:,2,:)) ;
    buttonPressed = qDiff > 0;
    rtRS(iM).avBP = nanmean(buttonPressed,5) ; % Button press average
    rtRS(iM).Q = Q;
    

    [rtRS(iM).rDistBP, rtRS(iM).pDistBP, rtRS(iM).hProxBP, aDQ, rDQ, cDQ] = ...
        CalcDistCorr(sFP,w,rtRS(iM).avBP);
    
    [rtRS(iM).rDistQ, rtRS(iM).pDistQ, rtRS(iM).hProxQ, aDQ, rDQ, cDQ] = ...
        CalcDistCorr(sFP,w,nanmean(rtRS(iM).Q,6));
end

[rMat_RS] = DisplayProxStats(rtRS);

% -------------------------------------------------------------------------
% Save variables necessary for outputting results [In FigureS3_Final.m]
save('Results\ForFigures\FigS3ab_Results.mat','rtRS')




%% COMBO MODEL: running the model with all the bells and whistles
%  [!!! WARNING: VERY SLOW !!!]
%  (i.e., Find performance of different network sizes for FULL EVERYTHING
%  MODEL)
clear s rS ntRS rtRS ytRS olRS

warning('Running this model is very slow. Better to download the precomputed version');

% -------------------------------------------------------------------------
% Base settings

% BODY
s.fl.bdyMov = 1;
s.act.bdyGoalRew = 2;
s.act.bdyThreatRew = -4;

% TOOL
s.fl.ToolChange = 1;
s.lmb.ToolRows = [5];
s.lmb.ToolProb = 0.5;

% THREAT
s.fl.thr = 1;
s.act.ThreatRew = -2;
s.act.DefaultMoveRew = -0.1;

% KINEMATICS
s.fl.hist = 0;
s.gol.alSpR = [1 2 3];
s.gol.alSpC = [-2 -1 0 1 2];
s.gol.randSpr = [2 2];
s.thr.alSpR = [1 2 3];
s.thr.alSpC = [-2 -1 0 1 2];
s.thr.randSpr = [0 3];


% LEARNING PARAMS
s.rp.maxActions = 4e6 ;
s.lp.minExp = 4e7; % minumum number of actions before training sarts (big here so no training)
s.lp.sWs=4e7; % minumum number of actions before saving sarts (big here so no training)
s.lp.dispA = 1e6; % only show action count very infreequently
s.lp.bSiz = 14e4;

% WORLD
s.wrld.size = [14 15];
s.wrld.resetType = 'BottomTop_InfLR';
s.lmb.startCol=8;

% RELEARNING 
s.rl.maxRetr=1;
s.fl.hist = 1;
s.fl.perfWhileLearn = 1;
s.prf.nRep = 10;
s.prf.skipBatches=25;
s.lp.ShwW = 1;
s.lp.mxF = 2;
s.rl.maxRetr=1; 
s.prf.skipBatches=1;
totalBatch = 1;
s.fl.newNet = 1;
s.lp.b1Siz = 14e4;
s.lp.bSiz = 14e4;

for iRep = 1:20
    
    % Run each type of model
    for iM = 1:length(netSizes)
        
        if iRep >1
            s.fl.newNet = 0;
        end
        s.fl.newTable = 0;
        
        % Change specific settings of the model
        s.lp.netS = netSizes{iM};
        
        % Relearn Q with different netsizes
        [net Qtable perf] = RelearnNNFun(s,w,storedEps,net);
        % Store network results Structure
        ntRS(iM).s=s; ntRS(iM).Qtable=Qtable; ntRS(iM).w=w; ntRS(iM).net=net;
        ntRS(iM).perf = perf;
        
        totalBatch = totalBatch + s.rl.maxRetr;
        
        bFld = 'Results\Performance\';
        svNm = [bFld 'ComboModel_Precomputed.mat'];
        save(svNm,'ntRS','-v7.3');
        
    end
    
end




%% EXTRA: Make videos showing what's happening during training

load('Results\Performance\ComboModel_Precomputed.mat')

iM                   = 1;
net                 = rS(iM).net;
s                   = DefaultSettings(rS(iM).s);
s.fl.newNet         = 0;
s.fl.dspm           = 1;
s.rp.maxActions     = 100;
s.fl.vid            = 1;
s.plt.fancyWorld    = 1;
s.plt.vidFR         = 7.5;
s.plt.vidFileName   = 'LearnVidLate.avi';
s.plt.grid          = 1;
[s2, w2, storedEps, ~, ~]    = RunRLfun(s,net);


% Also show pretraining
iM = 1;
net                 = rS(iM).net;
s                   = DefaultSettings(rS(iM).s);
s.fl.newNet         = 1;
s.fl.dspm           = 1;
s.rp.maxActions     = 100;
s.fl.vid            = 1;
s.plt.fancyWorld    = 1;
s.plt.vidFR         = 7.5;
s.plt.vidFileName   = 'LearnVidNoTraining.avi';
s.plt.grid          = 1;
[s2, w2, storedEps, ~, ~]    = RunRLfun(s);


%% FUNCTIONS


% -------------------------------------------------------------------------
% Functions for showing that one network can be generalised to another task

% Calculate error w.r.t. optimal data
function [sSqErr bestPsi] = ErrFun(baseQ, allQtoRecreate, w, cAct, s)

newQ = nansum(baseQ .* permute(w,[2 3 4 5 6 7 1]),7);

switch s.clc.maximiseSimilarityType
    case 'OverallQ'
        % Optimise weights over all actions, so sum over the actions and the tasks
        allSSqErr = nansum( (newQ - allQtoRecreate) .^ 2 , 5);
    case 'ChosenAction'
        [~, chosenAct]           = nanmax(newQ, [] ,5);
        [~, chosenActToRecreate] = nanmax(allQtoRecreate , [], 5);

        allSSqErr = nansum( ((chosenAct - chosenActToRecreate) ~= 0) , 5);

    case 'WinningQ' 
        [maxQs, chosenAct]   = nanmax( newQ, [] ,5);
        [maxQToRecreate, chosenActToRecreate] = nanmax(allQtoRecreate  , [], 5);
        
        % Compare to most valuable alternative
        winningQ            = zeros(size(maxQs));
        winningQToRecreate  = zeros(size(maxQToRecreate));
        for iR = 1:size(newQ,1)
            for iC = 1:size(newQ,2)
                for iRR = 1:size(newQ,3)
                    for iCC  = 1:size(newQ,4)
                        for iPol = 1:size(newQ,6)
                            otherActs = (1:size(newQ,5) ~= chosenAct(iR,iC,iRR,iCC,:,iPol));
                            winningQ(iR, iC, iRR, iCC,:,iPol)  = ...
                                maxQs(iR, iC, iRR, iCC,:,iPol) - ...
                                nanmax(newQ(iR, iC, iRR, iCC,otherActs,iPol), [], 5);


                            otherActsToRecreate = (1:size(newQ,5) ~= chosenActToRecreate(iR,iC,iRR,iCC));
                            winningQToRecreate(iR, iC, iRR, iCC,:) = ...
                                nanmaxQToRecreate(iR, iC, iRR, iCC,:) - ...
                                nanmax(allQtoRecreate(iR, iC, iRR, iCC,otherActsToRecreate), [], 5);
                        end
                    end
                end
            end
        end
        
     allSSqErr = nansum( (winningQ - winningQToRecreate).^2 , 5);
end
[minErrs, bestPsi] =  nanmin( allSSqErr , [] ,6);
sSqErr = nansum( minErrs , 'all');
end



% -------------------------------------------------------------------------
% Functions for fitting Coello 2018 and Zanini 2021

function [normalized_blurred_matrix] = GaussBlur(full_prob_matrix, kernel_size, sigma)

% Make it work for >2-dim arrays
normalized_blurred_matrix = full_prob_matrix;

full_prob_matrix = full_prob_matrix(:,:,:);

for iFurtherDim = 1:size(full_prob_matrix,3)
    prob_matrix = full_prob_matrix(:,:,iFurtherDim);
    % Create a Gaussian kernel
    [x, y] = meshgrid(-floor(kernel_size/2):floor(kernel_size/2), -floor(kernel_size/2):floor(kernel_size/2));
    if sigma == 0
        % Allow for there to be no smoothing, i.e. 0 sigma
        % (If we don't do this, we get NaN in the center of the kernel
        gaussian_kernel = double(x == 0 & y == 0);
    else
        gaussian_kernel = exp(-(x.^2 + y.^2) / (2 * sigma^2));
    end
    gaussian_kernel = gaussian_kernel / sum(gaussian_kernel(:)); % Normalize the kernel

    % Store the total probability of the original matrix
    total_prob = sum(prob_matrix(:));

    % Perform the convolution (2D Gaussian blur)
    blurred_matrix = conv2(prob_matrix, gaussian_kernel, 'same');

    % Normalize the blurred matrix to maintain the total probability
    if sum(blurred_matrix(:)) == 0 | total_prob == 0
        normalized_blurred_matrix(:,:,iFurtherDim) = 0;
    else
        normalized_blurred_matrix(:,:,iFurtherDim) = (blurred_matrix / sum(blurred_matrix(:))) * total_prob;
    end
end

end




function [handLocProbs, transProbs, onTableQPosX, onTableQPosY] = CalcReachProbabilities(s,allQ)


qOnTable = allQ(1,:).qVals{1}(:,:,:,2);


onTableQPosY = 62 - (1:size(qOnTable,2));
onTableQPosY = 5.* (  onTableQPosY - ((size(qOnTable,2)+1) - allQ(1,:).centerpos{1}(1))  );
onTableQPosX = 5.* (  (1:size(qOnTable,3)) - allQ(1,:).centerpos{1}(2)  );
[qX, qY]      = meshgrid(onTableQPosX,onTableQPosY);

handLocProbs = zeros(size(qX));
% Starting hand position is always at 0
handLocProbs(qX == 0 & qY == 0) = 1;
repmat(handLocProbs,[1 1 s.rch.nTP]);

% Also store transition probabilities
transProbs = zeros([size(qX), size(qX), s.rch.nTP]);


for iTP = 1:s.rch.nTP

    % loop through possible hand locations
    for iHLX = 1 : size(handLocProbs,2)
        for iHLY = 1 : size(handLocProbs,1)

            % Only calculate next possible hand locations if there is some
            % probability of being at the current hand location
            if handLocProbs(iHLY, iHLX, iTP) > 0

                % Reset the Q-values around the hand
                qAroundHand = zeros(size(qOnTable));

                % Find which positions fall 'on the table'
                nonZeroXVals = onTableQPosX >= s.rch.tableXMin - onTableQPosX(iHLX) & onTableQPosX <= s.rch.tableXMax - onTableQPosX(iHLX);
                nonZeroYVals = onTableQPosY >= s.rch.tableYMin - onTableQPosY(iHLY) & onTableQPosY <= s.rch.tableYMax - onTableQPosY(iHLY);
                qAroundHand(:,nonZeroYVals,nonZeroXVals) = qOnTable(:,nonZeroYVals,nonZeroXVals);

                % Change the values of the positions on the table to
                % reflect the experimental condition $$$
                farPositions  = onTableQPosY  > ((s.rch.tableYMax ./ 2) + s.rch.tableYMin - onTableQPosY(iHLY));
                nearPositions = onTableQPosY <= ((s.rch.tableYMax ./ 2) + s.rch.tableYMin - onTableQPosY(iHLY));
                qAroundHand(:, farPositions  ,:,:) =  qAroundHand(:, farPositions  ,:,:) .* s.rch.farVal ;
                qAroundHand(:, nearPositions ,:,:) =  qAroundHand(:, nearPositions ,:,:) .* s.rch.nearVal;

                % Remove the options to move in the z-plane
                qAroundHand(s.clc.actConsequence(:,3) ~= 0,:,:) = 0;

                % Pick the action stochastically
                % Mx across target positions first, then stochastic action selection?
                maxValAllLocs = max(qAroundHand,[],[2 3]);
                actProbs      = exp(maxValAllLocs ./ s.rch.temp) ./  sum( exp(maxValAllLocs ./ s.rch.temp) ,'all' );

                % Update future positions based on current actions
                for iAct = 1:size(actProbs,1)
                    % Find the effect of a given action
                    nextHandLocInd = [iHLY iHLX] - s.clc.actConsequence(iAct,1:2) - s.clc.baseVel(1:2);
                    % Deal with going out of bounds
                    nextHandLocInd(nextHandLocInd < 1) = 1;
                    if nextHandLocInd(1) > size(qAroundHand,2)
                        nextHandLocInd(1) = size(qAroundHand,2);
                    end
                    if nextHandLocInd(2) > size(qAroundHand,3)
                        nextHandLocInd(2) = size(qAroundHand,3);
                    end

                    % Multiply the action probability by the effect, and update the probability of ending
                    % up at a given location.
                    % Basically, this is future location probabilities, conditional on current location
                    transProbs(nextHandLocInd(1), nextHandLocInd(2), iHLY, iHLX, iTP)   =  actProbs(iAct)  ; % additional probabilities to end up in this point dure to the currently calculated action
                end

                %                 figure, imagesc(transProbs(:, :, iHLY, iHLX, iTP))
            end


        end
    end

    % Sum over all previous states, to find all future states

    handLocProbs(:,:,iTP + 1) = sum(transProbs(:,:,:,:, iTP) .* permute(handLocProbs(:, :, iTP),[4 5 1 2 3]) ,[3 4]);

end

end



function [outVals] = applyTDLearn(s)

    for iStep = 1 : s.nSteps + 1
        if iStep == 1
            outVals(:,iStep) = s.initV;
        else
            outVals(:,iStep) = (1 - s.alpha) .*                    outVals(:,iStep-1) + ...
                                    s.alpha .* (s.rew + s.gamma .* outVals(:,1));
        end
    end
    % Remove the initial value
    outVals(:,1) = [];

end

