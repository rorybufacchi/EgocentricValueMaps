% -----
% USE:
% 
% - Run this script to re-create:
% -- The elements used for making Figure S1 of the main paper
%
% - The data necessary for this figure is generated by CreateDataForPlots_Final.m,
% but a pre-computed version can also be loaded from:
% load('Results\ForFigures\ProximityPosition\WorkSpaceForProxPos.mat') $
% load('Results\ForFigures\ProximityPosition\WorkSpaceForProxPos_3.mat') $
% -----


% First make sure the path is set right
cFileName       = matlab.desktop.editor.getActiveFilename;
codePath        = cFileName(1:end-24);
addpath(genpath(codePath));


% NOTE: Change this to the file path where you store the generated data,
% whether downloaded or generated by yourself
dataPath        = '';
if              isempty(dataPath)
     dataPath       = SetPathEgocentricMapsData();
end
cd(             dataPath);

%% Load data
% NOTE: Load this if not computing the ANN outputs yourself
try
    load('Results\ForFigures\FigS1_Algorithm_Effects_Precomputed.mat');
catch
    warning('No Precomputed data found. Either Compute results using CreateDataForPlots.m, or download precompted results from $');
end


%% Effects of policy

f.Policy.f = figure('Position',[20 20 1800 900]);

sgtitle('POSITIVE VALENCE STIMULI                                                              NEGATIVE VALENCE STIMULI')

fS.lineYLim = [-0.01 0.55];
fS.lineXLim = [ 0   0.7];
fS.xPoints  = [5 5 ; 10 10]';
fS.cAxes    = [-2 2];

fS.gridXstart   = -5.5;
fS.gridXstep    =  1;
fS.gridYstart   =  2.5;
fS.gridYstep    =  1;

iPl = 0; iAx = 0;
for iD = 1:size(rS,2)

    % =====================================================================
    % Positive valence stimuli

    % -----------------------------------
    % Q-learning
    iPl = iPl + 1;
    iAx = iAx + 1;
    f.Policy.ax{iAx} = subplot(3,6,iPl);
    for iM = 1:3
        [tmpQ(:,:,:,:,:,iM),allNeurAct] = CalcNetOutput(rS(iM,iD).s,rS(iM,iD).w,rS(iM,iD).net);
    end
    Q = nanmean(tmpQ,[5 6]);
    [ax, plQhoriz(:,1,1,iD), plQvert(:,1,1,iD)] = plot2D(rS,fS,Q,iM,iD);
    axes(ax{1});
    caxis(fS.cAxes);
    title(['Q learning, policy update nr:' num2str(iD)]);

    % -----------------------------------
    % SARSA
    iPl = iPl+1;
    iAx = iAx + 1;
    f.Policy.ax{iAx} = subplot(3,6,iPl);
    for iM = 1:3
        [tmpQ(:,:,:,:,:,iM),allNeurAct] = CalcNetOutput(rSsarsa(iM,iD).s,rSsarsa(iM,iD).w,rSsarsa(iM,iD).net);
    end
    Q = nanmean(tmpQ,[5 6]);
    [ax, plQhoriz(:,1,2,iD), plQvert(:,1,2,iD)] = plot2D(rSsarsa,fS,Q,iM,iD);
    axes(ax{1});
    caxis(fS.cAxes);
    title(['SARSA, policy update nr:' num2str(iD)]);
   
    % -----------------------------------
    % SARSA with biased-direction epsilon-greedy policy
    iPl = iPl+1;
    iAx = iAx + 1;
    f.Policy.ax{iAx} = subplot(3,6,iPl);
    for iM = 1:3
        [tmpQ(:,:,:,:,:,iM),allNeurAct] = CalcNetOutput(rSsarsaUnb(iM,iD).s,rSsarsaUnb(iM,iD).w,rSsarsaUnb(iM,iD).net);
    end
    Q = nanmean(tmpQ,[5 6]);
    [ax, plQhoriz(:,1,3,iD), plQvert(:,1,3,iD)] = plot2D(rSsarsaUnb,fS,Q,iM,iD);
    axes(ax{1});
    caxis(fS.cAxes); 
    title(['UNbalanced SARSA, policy update nr:' num2str(iD)]);


    % =====================================================================
    % Negative valence stimuli

    % -----------------------------------
    % Q-learning
    iPl = iPl + 1;
    iAx = iAx + 1;
    f.Policy.ax{iAx} = subplot(3,6,iPl);
    for iM = 1:3
        [tmpQ(:,:,:,:,:,iM),allNeurAct] = CalcNetOutput(rSthr(iM,iD).s,rSthr(iM,iD).w,rSthr(iM,iD).net);
    end
    Q = nanmean(tmpQ,[5 6]);
    [ax, plQhoriz(:,2,1,iD), plQvert(:,2,1,iD)] = plot2D(rSthr,fS,Q,iM,iD);
    axes(ax{3}); xlim(-flip(fS.lineXLim));
    axes(ax{2}); ylim(-flip(fS.lineYLim));
    axes(ax{1}); 
    caxis(fS.cAxes);
    title(['Q learning, policy update nr:' num2str(iD)]);


    % -----------------------------------
    % SARSA
    iPl = iPl+1;
    iAx = iAx + 1;
    f.Policy.ax{iAx} = subplot(3,6,iPl);
    for iM = 1:3
        [tmpQ(:,:,:,:,:,iM),allNeurAct] = CalcNetOutput(rSsarsathr(iM,iD).s,rSsarsathr(iM,iD).w,rSsarsathr(iM,iD).net);
    end
    Q = nanmean(tmpQ,[5 6]);
    [ax, plQhoriz(:,2,2,iD), plQvert(:,2,2,iD)] = plot2D(rSsarsathr,fS,Q,iM,iD);
    axes(ax{3}); xlim(-flip(fS.lineXLim));
    axes(ax{2}); ylim(-flip(fS.lineYLim));
    axes(ax{1}); 
    caxis(fS.cAxes); 
    title(['SARSA, policy update nr:' num2str(iD)]);


    % -----------------------------------
    % SARSA with biased-direction epsilon-greedy policy
    iPl = iPl+1;
    iAx = iAx + 1;
    f.Policy.ax{iAx} = subplot(3,6,iPl);
    for iM = 1:3
        [tmpQ(:,:,:,:,:,iM),allNeurAct] = CalcNetOutput(rSsarsaUnbthr(iM,iD).s,rSsarsaUnbthr(iM,iD).w,rSsarsaUnbthr(iM,iD).net);
    end
    Q = nanmean(tmpQ,[5 6]);
    [ax, plQhoriz(:,2,3,iD), plQvert(:,2,3,iD)] = plot2D(rSsarsaUnbthr,fS,Q,iM,iD);
    axes(ax{3}); xlim(-flip(fS.lineXLim));
    axes(ax{2}); ylim(-flip(fS.lineYLim));
    axes(ax{1}); 
    caxis(fS.cAxes);
    title(['UNbalanced SARSA, policy update nr:' num2str(iD)]);

    colormap(flip(redbluecmapRory(11,5)))
end

% Plot the horizontal and vertical means in the same plots, to show effects

% pos, stimtype, algorithm, depth
vertPos = (size(plQvert, 1):-1:1) - 2;
horzPos = (size(plQhoriz,1):-1:1) - 7;

cDepths = 2;
algNames = {'Q-learning (optimisitc)', 'SARSA', 'left-biased SARSA'};
trainedOn = {'Initial random movement','1st policy-dependent movement','2nd policy-dependent movement'}
lineSettings = {'LineWidth',2};
f.PolicyLinesOptimism.f = figure('Position',[20 50 1200 1200]);
sgtitle('Q-learning gives more optimistic fields')

subplot(2,2,1)
plot(vertPos,  squeeze(  nanmean(plQvert(:,1,:,cDepths),4 )  ),lineSettings{:}); % Average over depth
title('Vertical distance; Positive stim: optimism results in bigger fields')
xlabel('Stim Dist')
ylabel('Q-value')
xlim([1 10]);
ylim([0 0.6]);
legend(algNames{:});
box off; hold on; grid on
plot([1 10],[0 0],'-k')

subplot(2,2,3)
plot(vertPos,  squeeze(  nanmean(plQvert(:,2,:,cDepths),4 )  ),lineSettings{:}); % Average over depth
title('Vertical distance; Negative stim: optimism results in smaller fields')
xlabel('Stim Dist')
ylabel('Q-value')
xlim([1 10]);
ylim([-0.6 0]);
box off; hold on; grid on
plot([1 10],[0 0],'-k')

subplot(2,2,2)
plot(horzPos,  squeeze(  nanmean(plQhoriz(:,1,:,cDepths),4 )  ),lineSettings{:}); % Average over depth
title('Horizontal distance; Positive stim: optimism results in bigger fields')
xlabel('Stim Dist')
ylabel('Q-value')
ylim([0 0.6]);
box off; hold on; grid on
plot([-6 6],[0 0],'-k')

subplot(2,2,4)
plot(horzPos,  squeeze(  nanmean(plQhoriz(:,2,:,cDepths),4 )  ),lineSettings{:}); % Average over depth
title('Horizontal distance; Negative stim: optimism results in smaller fields')
xlabel('Stim Dist')
ylabel('Q-value')
ylim([-0.6 0]);
box off; hold on; grid on
plot([-6 6],[0 0],'-k')


% LEARNING EFFECTS
f.PolicyLinesLearning.f = figure('Position',[20 50 1200 600]);


sgtitle('Effects of training and experience')

for iP = 1:length(algNames)

    subplot(2,6,iP)
    plot(vertPos,  squeeze(  plQvert(:,1,iP,:)  ),lineSettings{:}); 
    title( ['Vertical, ' algNames{iP} ])
    xlabel('Stim Dist')
    ylabel('Q-value: GOAL')
    box off
    grid on
    ylim([0 .6])
    xlim([1 10]);

    subplot(2,6,iP + 6)
    plot(vertPos,  squeeze(  plQvert(:,2,iP,:)  ),lineSettings{:}); 
    title( ['Vertical, ' algNames{iP} ])
    xlabel('Stim Dist')
    ylabel('Q-value: THREAT')
    box off
    grid on
    ylim([-.6 0])
    xlim([1 10]);


    subplot(2,6,iP + 3)
    plot(horzPos,  squeeze(  plQhoriz(:,1,iP,:)  ),lineSettings{:}); 
    title( ['Horizontal, ' algNames{iP} ])
    xlabel('Stim Dist')
    ylabel('Q-value: GOAL')
    box off
    grid on
    ylim([0 .6])

    subplot(2,6,iP + 9)
    plot(horzPos,  squeeze(  plQhoriz(:,2,iP,:)  ),lineSettings{:}); 
    title( ['Vertical, ' algNames{iP} ])
    xlabel('Stim Dist')
    ylabel('Q-value: THREAT')
    box off
    grid on
    ylim([-.6 0])

end
legend(trainedOn{:});


f.PolicyBarsLearning.f = figure('Position',[20 50 1200 600]);

% pos, stimtype, algorithm, depth
avQval = squeeze(nanmean(plQvert) + nanmean(plQhoriz));

for iP = 1:length(algNames)

    subplot(1,3,iP)
    bar(squeeze(avQval(:,iP,:)))
    set(gca,'XTickLabel',{'Goal','Threat'})
    ylabel('Average Q')
    title(algNames{iP})
    ylim([-0.5 0.5])
    grid on

end

legend(trainedOn)


f.Colourbars.f = figure('Position',[20 50 400 600]);
colorbar;
caxis(fS.cAxes);
colormap(flip(redbluecmapRory(11,5)))

%% Save figures

allFields = fields(f);
for iF = 1:length(allFields)
    cF = allFields{iF};
    
    set(f.(cF).f, 'Renderer', 'painters'); % default, opengl
    saveas(f.(cF).f,['Results\Outputs\FigureS1_' cF '.png'] , 'png')
    saveas(f.(cF).f,['Results\Outputs\FigureS1_' cF '.eps'] , 'epsc')
    saveas(f.(cF).f,['Results\Outputs\FigureS1_' cF '.pdf'] , 'pdf')
end


%% FUNCTIONS

function [ax, plQhoriz, plQvert] = plot2D(rS,fS,Q,iM,iD);
    
    rS(iM,iD).s.plt.contours = 1;
    rS(iM,iD).s.plt.contourVals = [-0.25, 0.25];
    
    rS(iM,iD).s.plt.plAct = 1;

    rS(iM,iD).s.plt.lmbCol = 2:14;
    rS(iM,iD).s.plt.meanLimbCols = 1;
    rS(iM,iD).s.plt.colLims = [.5 14.5];
    rS(iM,iD).s.plt.rowLims = [.5 14.5];
    DisplActValsFun(rS(iM,iD).s,rS(iM,iD).w,Q);
    colorbar off
    GridOverImage(fS,gca);
    xlim([-4.5 4.5]); ylim([1.5 13.5]);
    axis off

    ax{1} = gca;

    % Plot average OR line through Q-values
    ax{2} = axes('Position',ax{1}.Position .* [1 1 1 0] + [0 -0.05 0 0.05]  );
    plQhoriz = squeeze(nanmean(Q(1,8,2:11,2:end-1,:),[3 5])); % lmb R, C, stim R, C, A
    plot((1:size(Q,4))' - 0.5 , [0; plQhoriz; 0 ],'LineWidth',2); hold on
    xLims = xlim;
   xlim(xLims);
    hold on
    ylim(fS.lineYLim);
    grid on
    ax{2}.XAxis.Visible = 'off';
    ax{2}.YAxis.Visible = 'off';
    
    ax{3} = axes('Position',ax{1}.Position .* [1 1 0 1] + [-0.03 0 0.03 0]  );
    plQvert = squeeze(nanmean(Q(1,8,2:end-1,2:end-1,:),[4 5])); % lmb R, C, stim R, C, A
    plot( plQvert, (numel(plQvert):-1:1)' - 0.5 , 'LineWidth',2); hold on
    xlim(fS.lineXLim);
    grid on
    ax{3}.XAxis.Visible = 'off';
    ax{3}.YAxis.Visible = 'off';
 
end
