% -----
% USE:
% 
% - Run this script to re-create:
% -- The elements used for making Figure 4 of the main paper
% -- The statistical results related to Figure 4
%
% - The data necessary for this figure is generated by CreateDataForPlots_Final.m,
% !HOWEVER! a pre-computed version can also be loaded (much faster) from:
% load('Results\ForFigures\ProximityPosition\WorkSpaceForProxPos.mat') $
% load('Results\ForFigures\ProximityPosition\WorkSpaceForProxPos_3.mat') $
% -----

% First make sure the path is set right
cFileName                   = matlab.desktop.editor.getActiveFilename;
codePath                    = cFileName(1:end-24);
addpath(                    genpath(codePath));


% NOTE: Change this to the file path where you store the generated data,
% whether downloaded or generated by yourself
dataPath                    = ''; %'C:\Users\Rory Bufacchi\Documents\Projects\DefenseAgent'; '';
if                          isempty(dataPath)
     dataPath                   = SetPathEgocentricMapsData();
end
cd(                         dataPath);


%% Approximate Q-values from one task with rough Q-values or neural activity from another

% -------------------------------------------------------------------------
% Run a quick dummy agent to get the world-structure 'w' 
% (Needed for plotting later)
s.rp.maxActions                 = 10; 
s.wrld.size                     = [14 15];
s                               = DefaultSettings(s);
[~, w]                          = RunRLfun(s);
s                               = DefaultSettings(s);

% -------------------------------------------------------------------------
% Main settings
s.clc.maximiseSimilarityType = 'OverallQ'; %'WinningQ' ; %'ChosenAction'; % 'OverallQ'
allGammas = [0.7];
iGamma = 1;
s.clc.RewardBehindSurfaceFl = 0;
s.clc.checkCollisionFl      = 0;
s.wrld.size = [14 15 1];

% settings for plot
sFP=s;
sFP.plt.lmbRow = s.wrld.size(1)-2;
sFP.plt.rowLims=[6.5 s.wrld.size(1)-1.5];
sFP.plt.colLims=[3.5 s.wrld.size(2)-3.5];
sFP.plt.cBarFl=0;
sFP.plt.meanLimbCols=1;
sFP=DefaultSettings(sFP);
sFP.plt.axesVis=1;
fS.gridXstart = -4.5;
fS.gridXstep = 1;
fS.gridYstart = 3.5;
fS.gridYstep = 1;
sFP.plt.lmbCol=8;


% Data fitting variables
s.clc.startRew =  1;
s.clc.startSR = 12;
s.clc.startSC =  8;
s.clc.startSZ =  1;
s.clc.nearPos = [s.wrld.size(1)-0 8 1]';
s.clc.nReps = 1;
s.clc.stepUpdateFl = 0;
s.clc.nSteps = 1;

s.clc.gammaVal   = allGammas(iGamma);
s.clc.baseVel    = [1 0 0];

% Random stimulus dynamics (here: none)
rSpr = 0;
rSprPr = 1;
cSpr = 0;
cSprPr = 1;
zSpr = 0; 
zSprPr = 1;
% x y z, Specify deterministic stimulus dynamics
s.clc.stimDynams =     @(pos) pos + s.clc.baseVel; % For approaching, set speed positive
s.clc.randSpread =     {rSpr cSpr zSpr}; % Put a little biit of x and z variability in? Kind of arbitrary
s.clc.spreadProb =     {rSprPr cSprPr  zSprPr}; % x y z, probabilities of spread
% Random sensory uncertainties
s.clc.sensSpread = {[0] , [0] , [0]};
s.clc.sensProb   = {[1] , [1] , [1]};
% Consequences of actions
s.clc.actConsequence = ...
    [0  0  0 ; ... % action 1 stay
    0  1  0 ; ... % action 2 left
    0 -1  0];     % action 3 right

fS.cAxis = allGammas(iGamma) .* [-1 1] .* max(cSprPr);

clear baseTasks

% -------------------------------------------------------------------------
% Create Q-values for policies and tasks

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% BASE TASKS

% POLICY: grab goal
% --------------------------------
% Task: grab goal
s.clc.useAltPolicyFl = 0;
s.clc.startRew = 1;
polGtaskGQtmp = CalcHPDirect(s);
polGtaskGQ = repmat(permute(polGtaskGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{1,1}.allQ = polGtaskGQ;
baseTasks{1,1}.name = 'Pol: Goal, Task: Goal';

% Task: avoid threat
polGtaskTQ = -polGtaskGQ;
baseTasks{1,2}.allQ = polGtaskTQ;
baseTasks{1,2}.name = 'Pol: Goal, Task: Threat';

% Task: wide goal
sTmp = s;
sTmp.clc.useAltPolicyFl = 1;
sTmp.clc.startSC =  [ 7  8  9];
sTmp.clc.startSR =  [12 12 12];
sTmp.clc.startSZ =  [ 1  1  1];
sTmp.clc.startRew = 1;
polGtaskWideGQtmp = CalcHPDirect(sTmp, [], polGtaskGQtmp);
polGtaskWideGQ = repmat(permute(polGtaskWideGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{1,3}.allQ = polGtaskWideGQ;
baseTasks{1,3}.name = 'Pol: Goal,Task: Wide Goal';


% POLICY: avoid threat
% --------------------------------
% Task: avoid threat
s.clc.startRew = -1;
polTtaskTQtmp = CalcHPDirect(s);
polTtaskTQ = repmat(permute(polTtaskTQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{2,2}.allQ = polTtaskTQ;
baseTasks{2,2}.name = 'Pol: Threat, Task: Goal';

% Task: grab goal
polTtaskGQ = -polTtaskTQ;
baseTasks{2,1}.allQ = polTtaskGQ;
baseTasks{2,1}.name = 'Pol: Threat, Task: Threat';


% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% NOVEL TASKS

% Task: wide goal
sTmp = s;
sTmp.clc.useAltPolicyFl = 1;
sTmp.clc.startSC =  [ 7  8  9];
sTmp.clc.startSR =  [12 12 12];
sTmp.clc.startSZ =  [ 1  1  1];
sTmp.clc.startRew = 1;
polTtaskWideGQtmp = CalcHPDirect(sTmp, [], polTtaskTQtmp);
polTtaskWideGQ = repmat(permute(polTtaskWideGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{2,3}.allQ = polTtaskWideGQ ; %widepolGtaskTQ(:,:,:,:,1);
baseTasks{2,3}.name = 'Pol: Threat,Task: Wide Goal';


% POLICY: grab WIDE goal
% --------------------------------
% Task: wide goal
sTmp = s;
sTmp.clc.useAltPolicyFl = 0;
sTmp.clc.startSC =  [ 7  8  9];
sTmp.clc.startSR =  [12 12 12];
sTmp.clc.startSZ =  [ 1  1  1];
sTmp.clc.startRew = 1;
polWideGtaskWideGQtmp = CalcHPDirect(sTmp);
polWideGtaskWideGQ = repmat(permute(polWideGtaskWideGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{3,3}.allQ = polWideGtaskWideGQ;
baseTasks{3,3}.name = 'Pol: Wide Goal,Task: WideGoal';


% Task: grab goal
s.clc.useAltPolicyFl = 1;
s.clc.startRew = 1;
polWideGtaskGQtmp = CalcHPDirect(s,[], polWideGtaskWideGQtmp);
polWideGtaskGQ = repmat(permute(polWideGtaskGQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{3,1}.allQ = polWideGtaskGQ;
baseTasks{3,1}.name = 'Pol: Wide Goal, Task: Goal';

% Task: avoid threat
s.clc.useAltPolicyFl = 1;
s.clc.startRew = -1;
polWideGtaskTQtmp = CalcHPDirect(s,[], polWideGtaskWideGQtmp);
polWideGtaskTQ = repmat(permute(polWideGtaskTQtmp,[4 5 2 3 1]),[14 15 1 1]);
baseTasks{3,2}.allQ = polWideGtaskTQ;
baseTasks{3,2}.name = 'Pol: Wide Goal, Task: Threat';

% Reset alternative policy use
s.clc.useAltPolicyFl = 0;

% Create baseQ (matrix of basis tasks)
clear baseQ
for iPol = 1:size(baseTasks,1)
    qTmpForPol = arrayfun(@(iTask) baseTasks{iPol,iTask}.allQ, 1:size(baseTasks,2), 'UniformOutput', false);
    % row col row col act pol task
    baseQ(:,:,:,:,:,iPol,:) = cat(6,qTmpForPol{:});
end

% --------------------------------
% Task: 'stay in place for 1 timestep, then grab'
stationaryQ              = zeros(size(polGtaskGQ(:,:,:,:,:)));
stationaryQ(:,:,1:13,:,:)  = allGammas(iGamma).^1 .* polGtaskGQ(:,:,2:14,:,:);
stationaryQ(:,:,6:11,8,:)  = polGtaskGQ(:,:,6:11,8,:);
tasksToRecreate{1}.allQtoRecreate = stationaryQ; %stationaryQ(:,:,:,:,1);
tasksToRecreate{1}.name  = 'Stay1ThenGrab';

% --------------------------------
% Task: Avoid WIDE stimulus
s.clc.startSC =  [ 7  8  9];
s.clc.startSR =  [12 12 12];
s.clc.startSZ =  [ 1  1  1];

s.clc.startRew = -1;
widepolGtaskTQ = CalcHPDirect(s);
widepolGtaskTQ = repmat(permute(widepolGtaskTQ,[4 5 2 3 1]),[14 15 1 1]);
tasksToRecreate{2}.allQtoRecreate = widepolGtaskTQ; %widepolGtaskTQ(:,:,:,:,1);
tasksToRecreate{2}.name = 'WideThreat';
s.clc.startSC =  8;

% --------------------------------
% Task: Pass 2 blocks by the right of the stimulus
stationaryQ              = zeros(size(polGtaskGQ(:,:,:,:,:)));
stationaryQ(:,:,:,3:15,:)  = polGtaskGQ(:,:,:,1:13,:);
stationaryQ(:,:,:,1,:)     = polGtaskGQ(:,:,:,15,:);
tasksToRecreate{3}.allQtoRecreate = stationaryQ;  %stationaryQ(:,:,:,:,1);
tasksToRecreate{3}.name  = 'PassByRight2blocks';

% --------------------------------
% Task: flickering stimulus
flickerQ                 = polGtaskGQ(:,:,:,:,1);
flickerQ(:,:,1:2:end,:)  = polTtaskTQ(:,:,1:2:end,:,1); % polGtaskTQ(:,:,1:2:end,:,1);
tasksToRecreate{4}.allQtoRecreate = flickerQ; %flickerQ(:,:,:,:,1);
tasksToRecreate{4}.name     = 'FlickerStim';


% Plot the original Q-values
f.EgocentricTheory.f = figure('Position',[20 -20 1800 1800]),
sFP.act.Name = {'Stay','Left','Right','Up','Down','LeftUp','RightUp','LeftDown','RightDown'};
sFP.nCols = 3 + size(baseTasks,1) .* size(s.clc.actConsequence,1) ;
sFP.nRows = 5;
iCol = 1;
for iPol = 1:size(baseTasks,1)
    for iTask = 1:size(baseTasks,2)
        currQ = baseTasks{iPol,iTask}.allQ;
        for iAct = 1:size(currQ,5)
            subplot(sFP.nRows, sFP.nCols, sFP.nCols.*(iAct - 1) + iCol);
            sFP.plt.plAct = iAct;
            DisplActValsFun(sFP,w,currQ); hold on
            GridOverImage(fS,gca);
            colormap(flip(redbluecmapRory));
            caxis(fS.cAxis);
            title(['Q: ' sFP.act.Name{iAct}]);
            set(gca,'box','off');
            axis square
            title([ baseTasks{iPol,iTask}.name ]);
            if iCol == 1
                ylabel([ sFP.act.Name{iAct} ]);
            end
        end
        iCol = iCol + 1;
    end
end

sFP.plt.plAct = 1;
cAct = sFP.plt.plAct;

% -------------------------------------------------------------------------
% Plot the Q values to be fitted
for iFig = 1:numel(tasksToRecreate)

    allQtoRecreate  = tasksToRecreate{iFig}.allQtoRecreate; 
    allQtoPlot      = allQtoRecreate(:,:,:,:,cAct);

    subplot(sFP.nRows, sFP.nCols, sFP.nCols.*(iFig - 1) + size(baseTasks,1) .* size(s.clc.actConsequence,1) + 3 );
    DisplActValsFun(sFP,w,allQtoPlot); hold on
    GridOverImage(fS,gca);
    colormap(flip(redbluecmapRory));
    caxis(fS.cAxis);
    title(['Optimal Q for task: '  tasksToRecreate{iFig}.name])
    set(gca,'box','off')
    axis square
    
% % %     % ==================================================================
% % %     % FIT THE DATA BY JUST LINEARLY WEIGHTING ACTIONS (not identical to
% % %     % Barreto, but simple)
% % %     % Reshape polGtaskGQ for fitting
% % %     baseQtmp  = baseQ(:,:,:,:,:); % This version uses both tasks and individual actions as elements
% % %     baseQFlat = permute(baseQtmp, [5 1 2 3 4]);
% % %     baseQFlat = reshape(baseQFlat, size(baseQtmp,5), []);
% % % 
% % %     % Reshape allQtoRecreate
% % %     allQtoRecreateFlat = allQtoRecreate(:);
% % % 
% % %     corrFacts = (baseQFlat'\allQtoRecreateFlat);
% % %     fittedDataFlat = baseQFlat' * corrFacts;
% % % 
% % %     % Reshape the fitted data back to its original shape
% % %     fittedData = reshape(fittedDataFlat, 14, 15, 14, 15);
% % %     % ==================================================================

    % ==================================================================
    % FIT THE DATA MORE CLOSE TO THE BARRETO METHOD (i.e. don't use actions
    % as separate features)
    % Define function to be optimised
    FunToOpt = @(p) ErrFun(baseQ, allQtoRecreate, p, cAct, sFP);

    % Set optimisation options - keep it simple
    A = []; b = []; Aeq = []; beq = []; a = tic;
    w0 = ones([size(baseQ,7), 1]);
    lb = -[Inf Inf]';
    ub = [Inf Inf]';
    % Run optimisation
    OPTIONS = optimset('TolCon',1e-10);
    [p,sSqErr,exitflag,output,lambda,grad,hessian] = fmincon(FunToOpt,w0,A,b,Aeq,beq,lb,ub,[],OPTIONS);
    optTime = toc(a)

    % Extract optimised data
    [sSqErrFinal, bestPsi] = ErrFun(baseQ, allQtoRecreate, p, cAct, sFP);
    % fittedData =

    fittedData   = zeros(size(baseQ(:,:,:,:,1)));
    weightedData = sum(baseQ(:,:,:,:,cAct,:,:,:) .* permute(p,[2 3 4 5 6 7 1]),7);
    for r = 1:size(bestPsi,1)
        for c = 1:size(bestPsi,2)
            for rr = 1:size(bestPsi,3)
                for cc = 1:size(bestPsi,4)

                    % Select the correct policy for each condition
                    fittedData(r,c,rr,cc) = weightedData(r,c,rr,cc,bestPsi(r,c,rr,cc));

                end
            end
        end
    end
    % ==================================================================


    subplot(sFP.nRows, sFP.nCols, sFP.nCols.*(iFig - 1) + size(baseTasks,1) .* size(s.clc.actConsequence,1) + 2  );
    DisplActValsFun(sFP,w,fittedData); hold on
    GridOverImage(fS,gca);
    colormap(flip(redbluecmapRory));
    caxis(fS.cAxis)
    title(['Approximation :' sprintf('%.2f  ' , p)] );
    set(gca,'box','off')
    axis square

    % Plot the chosen policy
    subplot(sFP.nRows, sFP.nCols, sFP.nCols.*(iFig - 1) + size(baseTasks,1) .* size(s.clc.actConsequence,1) + 1 );
    DisplActValsFun(sFP,w,bestPsi); hold on
    GridOverImage(fS,gca);
    title(['Optimal Psi'] );
    set(gca,'box','off')
    axis square

end

colormap(flip(redbluecmapRory));

% Go through the optimal policies and change colour maps locally
for iFig = 1:numel(tasksToRecreate)
            subplot(sFP.nRows, sFP.nCols, sFP.nCols.*(iFig - 1) + size(baseTasks,1) .* size(s.clc.actConsequence,1) + 1  );
            colormap(gca,[.3 .3 .8 ; .8 .3 .3; 0 0 .6])
end




%% Save figures

allFields                   = fields(f);
for iF                      = 1:length(allFields)
    cF                          = allFields{iF};
    set(                        f.(cF).f, 'Renderer', 'painters'); % default, opengl
    saveas(                     f.(cF).f,['Results\Outputs\Figure4_' cF '.png'] , 'png')
    saveas(                     f.(cF).f,['Results\Outputs\Figure4_' cF '.eps'] , 'epsc')
    saveas(                     f.(cF).f,['Results\Outputs\Figure4_' cF '.pdf'] , 'pdf')
end


%% FUNCTIONS


% Calculate error w.r.t. optimal data
function [sSqErr bestPsi] = ErrFun(baseQ, allQtoRecreate, w, cAct, s)

newQ = nansum(baseQ .* permute(w,[2 3 4 5 6 7 1]),7);

switch s.clc.maximiseSimilarityType
    case 'OverallQ'
        % Optimise weights over all actions, so sum over the actions and the tasks
        allSSqErr = nansum( (newQ - allQtoRecreate) .^ 2 , 5);
    case 'ChosenAction'
        [~, chosenAct]           = nanmax(newQ, [] ,5);
        [~, chosenActToRecreate] = nanmax(allQtoRecreate , [], 5);

        allSSqErr = nansum( ((chosenAct - chosenActToRecreate) ~= 0) , 5);

    case 'WinningQ' 
        [maxQs, chosenAct]   = nanmax( newQ, [] ,5);
        [maxQToRecreate, chosenActToRecreate] = nanmax(allQtoRecreate  , [], 5);
        
        % Compare to most valuable alternative
        winningQ            = zeros(size(maxQs));
        winningQToRecreate  = zeros(size(maxQToRecreate));
        for iR = 1:size(newQ,1)
            for iC = 1:size(newQ,2)
                for iRR = 1:size(newQ,3)
                    for iCC  = 1:size(newQ,4)
                        for iPol = 1:size(newQ,6)
                            otherActs = (1:size(newQ,5) ~= chosenAct(iR,iC,iRR,iCC,:,iPol));
                            winningQ(iR, iC, iRR, iCC,:,iPol)  = ...
                                maxQs(iR, iC, iRR, iCC,:,iPol) - ...
                                nanmax(newQ(iR, iC, iRR, iCC,otherActs,iPol), [], 5);


                            otherActsToRecreate = (1:size(newQ,5) ~= chosenActToRecreate(iR,iC,iRR,iCC));
                            winningQToRecreate(iR, iC, iRR, iCC,:) = ...
                                nanmaxQToRecreate(iR, iC, iRR, iCC,:) - ...
                                nanmax(allQtoRecreate(iR, iC, iRR, iCC,otherActsToRecreate), [], 5);
                        end
                    end
                end
            end
        end
        
     allSSqErr = nansum( (winningQ - winningQToRecreate).^2 , 5);
end
[minErrs, bestPsi] =  nanmin( allSSqErr , [] ,6);
sSqErr = nansum( minErrs , 'all');
end
